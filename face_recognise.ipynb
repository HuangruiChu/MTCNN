{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 人脸识别"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "代码来源https://github.com/GitHberChen/MTCNN_Pytorch\n",
    "\n",
    "查阅约20篇关于MTCNN的实现文章，最终选取这个版本的加以改编。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#基本的引入，按字母表排序\n",
    "import argparse\n",
    "import cv2\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "from numpy.random import uniform\n",
    "import os\n",
    "from os import path as osp\n",
    "import PIL\n",
    "from PIL import Image, ImageDraw\n",
    "import random\n",
    "from random import shuffle\n",
    "import sys#####不知道这个是干什么的\n",
    "import time\n",
    "#torch部分的import\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as opt\n",
    "#from tensorboardX import SummaryWriter\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "#warning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 设置随机数种子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "# 设置随机数种子\n",
    "setup_seed(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 设置args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 原文作者通过arg减少了很多重复的语句，而且在args上可以很快的更改一些数据。其便利性让我沿用这个写法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "global device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if torch.cuda.is_available():\n",
    "    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "    cudnn.benchmark = True\n",
    "    cudnn.deterministic = True\n",
    "else:\n",
    "    torch.set_default_tensor_type('torch.FloatTensor')\n",
    "    # torch.set_default_tensor_type('torch.DoubleTensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def config():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    #training_data的路径设置\n",
    "    parser.add_argument('--class_traindata_txt_path',\n",
    "                        default='../input/wider-face-recognization/wider_face_split/wider_face_split/wider_face_train_bbx_gt.txt',\n",
    "                        type=str, help='the path of wider_face_train_bbx_gt.txt')\n",
    "    parser.add_argument('--class_traindata_dir', default='../input/wider-face-recognization/WIDER_train/WIDER_train/images',\n",
    "                        type=str, help='the dir of WILDER FACE train image file')\n",
    "    parser.add_argument('--landmark_traindata_txt_path',\n",
    "                        default=None, type=str, help='the path of CelebA .txt file')\n",
    "    parser.add_argument('--landmark_traindata_dir', \n",
    "                        default=None, type=str,help='the dir of CelebA image file')\n",
    "    #valid_data的路径设置\n",
    "    parser.add_argument('--class_validdata_txt_path',\n",
    "                        default='../input/wider-face-recognization/wider_face_split/wider_face_split/wider_face_val_bbx_gt.txt',\n",
    "                        type=str, help='the path of wider_face_train_bbx_gt.txt')\n",
    "    parser.add_argument('--class_validdata_dir', default='../input/wider-face-recognization/WIDER_val/WIDER_val/images',\n",
    "                        type=str, help='the dir of WILDER FACE valid image file')\n",
    "    \n",
    "    parser.add_argument('--landmark_validdata_txt_path',\n",
    "                        default=None, type=str, help='the path of CelebA .txt file')\n",
    "    parser.add_argument('--landmark_validdata_dir', \n",
    "                        default=None, type=str,help='the dir of CelebA image file')\n",
    "    #数据增强设置\n",
    "    parser.add_argument('--class_data_augment', default=3,\n",
    "                        type=int, help='the augment ratio for create pnet data set')\n",
    "    #储存路径设置\n",
    "    parser.add_argument('--save_folder', type=str,default='/kaggle/working',  \n",
    "                        help='the folder of p/r/onet_para.pkl, p/r/onet.pkl saved')\n",
    "    parser.add_argument('--train_net', type=str,\n",
    "                        default='pnet', choices=['pnet', 'rnet', 'onet'],\n",
    "                        help='choose net to train')\n",
    "    \n",
    "    #超参数设置\n",
    "    parser.add_argument('--lr', type=float,\n",
    "                        default=0.001,\n",
    "                        help='initial learning rate')\n",
    "    parser.add_argument('--epoch', type=int,\n",
    "                        default=50,\n",
    "                        help='some batches make up a sub_epoch ')\n",
    "    parser.add_argument('--batch_size', type=int,\n",
    "                        default=512,\n",
    "                        help='batch_size ')\n",
    "    #这里的num——worker数量如果不为零，且使用GPU，是会报错的。\n",
    "    parser.add_argument('--num_workers', type=int,\n",
    "                        default=0,\n",
    "                        help='workers for loading the data')\n",
    "    parser.add_argument('--half_lr_steps', type=int,\n",
    "                        default=100,\n",
    "                        help='half the lr every half_lr_steps iter')\n",
    "    parser.add_argument('--save_steps', type=int,\n",
    "                        default=10,\n",
    "                        help='save para, model every save_steps iter')\n",
    "    \n",
    "    args = parser.parse_args(args=[])\n",
    "\n",
    "    return args\n",
    "\n",
    "args=config()\n",
    "#后面的Loss的内置系数\n",
    "net_loss_config = {\n",
    "    'pnet': [1.0, 0.5, 0.5],\n",
    "    'rnet': [1.0, 0.5, 0.5],\n",
    "    'onet': [1.0, 0.5, 1.0]\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nms(boxes, overlap_threshold=0.5, mode='union'):\n",
    "    \"\"\" Pure Python NMS baseline. \"\"\"\n",
    "    x1 = boxes[:, 0]\n",
    "    y1 = boxes[:, 1]\n",
    "    x2 = boxes[:, 2]\n",
    "    y2 = boxes[:, 3]\n",
    "    scores = boxes[:, 4]\n",
    "\n",
    "    areas = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
    "    # argsort()默认从小到大排序，取反后就是从大到小\n",
    "    order = scores.argsort()[::-1]\n",
    "\n",
    "    keep = []\n",
    "    while order.size > 0:\n",
    "        i = order[0]\n",
    "        keep.append(i)\n",
    "        xx1 = np.maximum(x1[i], x1[order[1:]])\n",
    "        yy1 = np.maximum(y1[i], y1[order[1:]])\n",
    "        xx2 = np.minimum(x2[i], x2[order[1:]])\n",
    "        yy2 = np.minimum(y2[i], y2[order[1:]])\n",
    "\n",
    "        w = np.maximum(0.0, xx2 - xx1 + 1)\n",
    "        h = np.maximum(0.0, yy2 - yy1 + 1)\n",
    "        inter = w * h\n",
    "\n",
    "        if mode is 'min':\n",
    "            ovr = inter / np.minimum(areas[i], areas[order[1:]])\n",
    "        else:\n",
    "            ovr = inter / (areas[i] + areas[order[1:]] - inter)\n",
    "\n",
    "        inds = np.where(ovr <= overlap_threshold)[0]\n",
    "        order = order[inds + 1]\n",
    "    # print(keep)\n",
    "    return keep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_square(bboxes):\n",
    "    \"\"\"\n",
    "    Convert bounding boxes to a square form.\n",
    "    \"\"\"\n",
    "    # 将矩形对称扩大为正方形\n",
    "    square_bboxes = np.zeros_like(bboxes)\n",
    "    x1, y1, x2, y2 = [bboxes[:, i] for i in range(4)]\n",
    "    h = y2 - y1 + 1.0\n",
    "    w = x2 - x1 + 1.0\n",
    "    max_side = np.maximum(h, w)\n",
    "    square_bboxes[:, 0] = x1 + w * 0.5 - max_side * 0.5\n",
    "    square_bboxes[:, 1] = y1 + h * 0.5 - max_side * 0.5\n",
    "    square_bboxes[:, 2] = square_bboxes[:, 0] + max_side - 1.0\n",
    "    square_bboxes[:, 3] = square_bboxes[:, 1] + max_side - 1.0\n",
    "    return square_bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calibrate_box(bboxes, offsets):\n",
    "    \"\"\"\n",
    "        Transform bounding boxes to be more like true bounding boxes.\n",
    "        'offsets' is one of the outputs of the nets.\n",
    "    \"\"\"\n",
    "    x1, y1, x2, y2 = [bboxes[:, i] for i in range(4)]\n",
    "    w = x2 - x1 + 1.0\n",
    "    h = y2 - y1 + 1.0\n",
    "    # w [w_len, 1]\n",
    "    w = np.expand_dims(w, 1)\n",
    "    # h [h_len, 1]\n",
    "    h = np.expand_dims(h, 1)\n",
    "\n",
    "    translation = np.hstack([w, h, w, h]) * offsets\n",
    "    bboxes[:, 0:4] = bboxes[:, 0:4] + translation\n",
    "    return bboxes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_boxes(bounding_boxes, img, size=24):\n",
    "    \"\"\" Cut out boxes from the image. \"\"\"\n",
    "    num_boxes = len(bounding_boxes)\n",
    "    # print('bounding_boxes:', bounding_boxes)\n",
    "    width, height = img.size\n",
    "\n",
    "    [dy, edy, dx, edx, y, ey, x, ex, w, h] = correct_bboxes(bounding_boxes, width, height)\n",
    "    img_boxes = np.zeros((num_boxes, 3, size, size), 'float32')\n",
    "\n",
    "    for i in range(num_boxes):\n",
    "        img_box = np.zeros((h[i], w[i], 3), 'uint8')\n",
    "\n",
    "        img_array = np.asarray(img, 'uint8')\n",
    "        # print('img_array.shape:', img_array.shape)\n",
    "        img_box[dy[i]:(edy[i] + 1), dx[i]:(edx[i] + 1), :] = \\\n",
    "            img_array[y[i]:(ey[i] + 1), x[i]:(ex[i] + 1), :]\n",
    "\n",
    "        img_box = Image.fromarray(img_box)\n",
    "        img_box = img_box.resize((size, size), Image.BILINEAR)\n",
    "        img_box = np.asarray(img_box, 'float32')\n",
    "\n",
    "        img_boxes[i, :, :, :] = img_normalization(img_box)\n",
    "\n",
    "    return img_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_bboxes(bboxes, width, height):\n",
    "    \"\"\"\n",
    "        Crop boxes that are too big and get coordinates\n",
    "    with respect to cutouts.\n",
    "    \"\"\"\n",
    "    x1, y1, x2, y2 = [bboxes[:, i] for i in range(4)]\n",
    "    w, h = x2 - x1 + 1.0, y2 - y1 + 1.0\n",
    "    num_boxes = bboxes.shape[0]\n",
    "\n",
    "    x, y, ex, ey = x1, y1, x2, y2\n",
    "    dx, dy = np.zeros((num_boxes,)), np.zeros((num_boxes,))\n",
    "    edx, edy = w.copy() - 1.0, h.copy() - 1.0\n",
    "\n",
    "    ind = np.where(ex > width - 1.0)[0]\n",
    "    edx[ind] = w[ind] + width - 2.0 - ex[ind]\n",
    "    ex[ind] = width - 1.0\n",
    "\n",
    "    ind = np.where(ey > height - 1.0)[0]\n",
    "    edy[ind] = h[ind] + height - 2.0 - ey[ind]\n",
    "    ey[ind] = height - 1.0\n",
    "\n",
    "    ind = np.where(x < 0.0)[0]\n",
    "    dx[ind] = 0.0 - x[ind]\n",
    "    x[ind] = 0.0\n",
    "\n",
    "    ind = np.where(y < 0.0)[0]\n",
    "    dy[ind] = 0.0 - y[ind]\n",
    "    y[ind] = 0.0\n",
    "    return_list = [dy, edy, dx, edx, y, ey, x, ex, w, h]\n",
    "    return_list = [i.astype('int32') for i in return_list]\n",
    "\n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_normalization(img):\n",
    "    \"\"\"Preprocessing step before feeding the network. \"\"\"\n",
    "    img = img.transpose((2, 0, 1))\n",
    "    img = np.expand_dims(img, 0)\n",
    "    # *0.0078125 i.e. 除以128\n",
    "    img = (img - 127.5) * 0.0078125\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IoU(box, boxes):\n",
    "    \"\"\"\n",
    "    Compute IoU between detect box and gt boxes\n",
    "    \"\"\"\n",
    "    # box = (x1, y1, x2, y2)\n",
    "    box_area = (box[2] - box[0] + 1) * (box[3] - box[1] + 1)\n",
    "    area = (boxes[:, 2] - boxes[:, 0] + 1) * (boxes[:, 3] - boxes[:, 1] + 1)\n",
    "\n",
    "    # abtain the offset of the interception of union between crop_box and gt_box\n",
    "    xx1 = np.maximum(box[0], boxes[:, 0])\n",
    "    yy1 = np.maximum(box[1], boxes[:, 1])\n",
    "    xx2 = np.minimum(box[2], boxes[:, 2])\n",
    "    yy2 = np.minimum(box[3], boxes[:, 3])\n",
    "\n",
    "    # compute the width and height of the bounding box\n",
    "    w = np.maximum(0, xx2 - xx1 + 1)\n",
    "    h = np.maximum(0, yy2 - yy1 + 1)\n",
    "\n",
    "    inter = w * h\n",
    "    ovr = inter / (box_area + area - inter)\n",
    "    return ovr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_bboxes(img, bounding_boxes, facial_landmarks=[]):\n",
    "    \"\"\" Draw bounding boxes and facial landmarks. \"\"\"\n",
    "    img_copy = img.copy()\n",
    "    draw = ImageDraw.Draw(img_copy)\n",
    "\n",
    "    for b in bounding_boxes:\n",
    "        draw.rectangle([(b[0], b[1]), (b[2], b[3])],\n",
    "                       outline='red')\n",
    "\n",
    "    for p in facial_landmarks:\n",
    "        # print(p)\n",
    "        for i in range(5):\n",
    "            draw.ellipse([(p[i] - 1.0, p[i + 5] - 1.0),\n",
    "                          (p[i] + 1.0, p[i + 5] + 1.0)],\n",
    "                         outline='green')\n",
    "    return img_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_first_stage(image, net, scale, threshold):\n",
    "    \n",
    "    from torch.multiprocessing import Pool, Process, set_start_method, cpu_count\n",
    "    try:\n",
    "        set_start_method('spawn')\n",
    "    except RuntimeError:\n",
    "        pass\n",
    "    \"\"\" \n",
    "        Run P-Net, generate bounding boxes, and do NMS.\n",
    "    \"\"\"\n",
    "    width, height = image.size\n",
    "    sw, sh = math.ceil(width * scale), math.ceil(height * scale)\n",
    "    img = image.resize((sw, sh), Image.BILINEAR)\n",
    "    # img = np.asarray(img, 'float32')\n",
    "    # preprocess 对图像进行归一化操作\n",
    "    img = transforms.ToTensor()(img).unsqueeze(0)\n",
    "    img = img.to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "    # print('img:', img)\n",
    "\n",
    "    output = net(img)\n",
    "    # 只有一张图 batch = 1，所以 [0, ,:,:]\n",
    "    # [ , 1,:,:]代表 face=True 的概率\n",
    "    probs = output[0].data.cpu().numpy()[0, 0, :, :]\n",
    "    # offsets shape[4, o_h,o_w]\n",
    "    offsets = output[1].data.cpu().numpy()\n",
    "    # print('offsets:', offsets)\n",
    "    # boxes\n",
    "    boxes = _generate_bboxes(probs, offsets, scale, threshold)\n",
    "    if len(boxes) == 0:\n",
    "        return None\n",
    "\n",
    "    # [[x1,y1,x2,y2,score,offsets],[]...]\n",
    "    # 只取4个坐标加一个置信度进行nms\n",
    "    keep = nms(boxes[:, 0:5], overlap_threshold=0.5)\n",
    "    return boxes[keep]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _generate_bboxes(probs, offsets, scale, threshold):\n",
    "    \"\"\"\n",
    "       Generate bounding boxes at places where there is probably a face.\n",
    "    \"\"\"\n",
    "    stride = 2\n",
    "    cell_size = 12\n",
    "\n",
    "    # inds = output_feature_map [ :, :], 坐标\n",
    "    inds = np.where(probs > threshold)\n",
    "    '''\n",
    "    >>> a =np.array([[1,2,3],[4,5,6]])\n",
    "    >>> np.where(a>1)\n",
    "    (array([0, 0, 1, 1, 1]), array([1, 2, 0, 1, 2]))\n",
    "    '''\n",
    "    # print('face candidate num'.format(len(inds)))\n",
    "    if inds[0].size == 0:\n",
    "        return np.array([])\n",
    "    # offsets shape[4, o_h,o_w]\n",
    "    tx1, ty1, tx2, ty2 = [offsets[0, i, inds[0], inds[1]] for i in range(4)]\n",
    "    # for i in zip(tx1, ty1, tx2, ty2):\n",
    "    #     print([i[j] for j in range(4)])\n",
    "\n",
    "    offsets = np.array([tx1, ty1, tx2, ty2])\n",
    "    score = probs[inds[0], inds[1]]\n",
    "    # print('score:', score)\n",
    "\n",
    "    # P-Net is applied to scaled images, so we need to rescale bounding boxes back\n",
    "    bounding_boxes = np.vstack([\n",
    "        np.round((stride * inds[1] + 1.0) / scale),\n",
    "        np.round((stride * inds[0] + 1.0) / scale),\n",
    "        np.round((stride * inds[1] + 1.0 + cell_size) / scale),\n",
    "        np.round((stride * inds[0] + 1.0 + cell_size) / scale),\n",
    "        score, offsets\n",
    "    ])\n",
    "    # from\n",
    "    # [[x1,x1,...]\n",
    "    #  [y1,y1,...]\n",
    "    #  [x2,x2,...]\n",
    "    #  [y2,y2,...]\n",
    "    # ]to\n",
    "    # [[x1,y1,x2,y2,score,offsets],[]...]\n",
    "    # shape[9,boxes_num]\n",
    "    # print(bounding_boxes.shape)\n",
    "    # print(bounding_boxes.T.shape)\n",
    "    return bounding_boxes.T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PNet-Box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLDS = [0.6, 0.7, 0.8]\n",
    "NMS_THRESHOLDS = [0.9, 0.9, 0.3]\n",
    "MIN_FACE_SIZE = 25.0\n",
    "def pnet_boxes(img, pnet, min_face_size=MIN_FACE_SIZE, thresholds=THRESHOLDS, nms_thresholds=NMS_THRESHOLDS,\n",
    "               show_boxes=True):\n",
    "    pnet.eval()\n",
    "    width, height = img.size\n",
    "    min_length = min(height, width)\n",
    "    # print('img min_length is {}'.format(min_length))\n",
    "    min_detection_size = 12\n",
    "    factor = 0.707  # sqrt(0.5)\n",
    "    scales = []\n",
    "    m = min_detection_size / min_face_size\n",
    "    # 缩放原图使得最小脸尺寸为12pix\n",
    "    min_length *= m\n",
    "    factor_count = 0\n",
    "    while min_length > min_detection_size:\n",
    "        scales.append(m * factor ** factor_count)\n",
    "        min_length *= factor\n",
    "        factor_count += 1\n",
    "\n",
    "    # STAGE 1\n",
    "    bounding_boxes = []\n",
    "    for s in scales:  # run P-Net on different scales\n",
    "        boxes = run_first_stage(img, pnet, scale=s, threshold=thresholds[0])\n",
    "        bounding_boxes.append(boxes)\n",
    "        # bounding_boxes shape:[scales,boxes_num_each_sale,5]\n",
    "    # 把每个scale找到的框框全部打开堆在一起\n",
    "    # [total_boxes_num, 5] 是list\n",
    "    bounding_boxes = [i for i in bounding_boxes if i is not None]\n",
    "    # print(bounding_boxes)\n",
    "    # bounding_boxes = np.array(bounding_boxes)\n",
    "    # print(bounding_boxes.shape, img.size)\n",
    "    try:\n",
    "        _ = bounding_boxes[0]\n",
    "        # print('bounding_boxes:{}'.format(len(bounding_boxes)))\n",
    "        # print('bounding_boxes[0]:{}'.format(len(bounding_boxes[0])))\n",
    "    except Exception:\n",
    "#         print(bounding_boxes)\n",
    "        img.show()\n",
    "    if len(bounding_boxes) == 0:\n",
    "        return None\n",
    "    bounding_boxes = np.vstack(bounding_boxes)\n",
    "    # print(bounding_boxes.shape)\n",
    "\n",
    "    keep = nms(bounding_boxes[:, 0:5], nms_thresholds[0])\n",
    "    bounding_boxes = bounding_boxes[keep]\n",
    "    # print('bounding_boxes:{}'.format(bounding_boxes[:, 4] > 0.5))\n",
    "    # 根据 w、h 对 x1,y1,x2,y2 的位置进行微调\n",
    "    bounding_boxes = calibrate_box(bounding_boxes[:, 0:5], bounding_boxes[:, 5:])\n",
    "    # 将检测出的框转化成矩形\n",
    "    bounding_boxes = convert_to_square(bounding_boxes)\n",
    "    bounding_boxes[:, 0:4] = np.round(bounding_boxes[:, 0:4])\n",
    "    # print('bounding_boxes:{}'.format(bounding_boxes[:, 4] > 0.5))\n",
    "    # print('bounding_boxes:', len(bounding_boxes), bounding_boxes)\n",
    "    if show_boxes: show_bboxes(img, bounding_boxes, []).show()\n",
    "    return bounding_boxes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNet-Box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnet_boxes(img, rnet, bounding_boxes, thresholds=THRESHOLDS, nms_thresholds=NMS_THRESHOLDS, show_boxes=True):\n",
    "    rnet.eval()\n",
    "    img_boxes = get_image_boxes(bounding_boxes, img, size=24)\n",
    "    img_boxes = torch.FloatTensor(img_boxes)\n",
    "    img_boxes=img_boxes.to(device)\n",
    "#     img_boxes=img_boxes.cuda()\n",
    "    # img_boxes = img_boxes.to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "    output = rnet(img_boxes)\n",
    "    probs = output[0].data.cpu().numpy()  # shape [n_boxes, 1]\n",
    "    offsets = output[1].data.cpu().numpy()  # shape [n_boxes, 4]\n",
    "\n",
    "    keep = np.where(probs[:, 0] > thresholds[1])[0]\n",
    "    bounding_boxes = bounding_boxes[keep]\n",
    "    bounding_boxes[:, 4] = probs[keep, 0].reshape((-1,))\n",
    "    offsets = offsets[keep]\n",
    "\n",
    "    keep = nms(bounding_boxes, nms_thresholds[1])\n",
    "    bounding_boxes = bounding_boxes[keep]\n",
    "    bounding_boxes = calibrate_box(bounding_boxes, offsets[keep])\n",
    "    bounding_boxes = convert_to_square(bounding_boxes)\n",
    "    bounding_boxes[:, 0:4] = np.round(bounding_boxes[:, 0:4])\n",
    "    if show_boxes: show_bboxes(img, bounding_boxes, []).show()\n",
    "    return bounding_boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 图片的预处理："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.对要输入P-Net图片的预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pnet_data_txt_parser(txt_path, img_dir):\n",
    "    \"\"\"\n",
    "    :param txt_path: the path of wider_face_train_bbx_gt.txt\n",
    "    :param img_dir: tha dir of WIDER/WIDER_train\n",
    "    :return: img_faces type is list, shape is [img_num*[absolute_img_path,[faces_num*4(which is x1,y1,w,h)]]]\n",
    "    \"\"\"\n",
    "    if osp.exists(txt_path):\n",
    "        # *** img_faces shape :[img_path,[faces_num, 4]]\n",
    "        img_faces = []\n",
    "        with open(txt_path, 'r') as f:\n",
    "            l = []\n",
    "            lines = list(map(lambda line: line.strip().split('\\n'), f))\n",
    "            # lines[[str],[str],[]...]\n",
    "            lines = [i[0] for i in lines]\n",
    "            # lines [str,str...]\n",
    "            line_counter = 0\n",
    "            img_count=0\n",
    "            while line_counter < len(lines):\n",
    "                img_path = lines[line_counter]\n",
    "                img_count+=1\n",
    "                faces_pos = []\n",
    "                faces_num = int(lines[line_counter + 1])\n",
    "                if faces_num==0:\n",
    "                    print(\"Find a picture with no face in it.\")\n",
    "                    line_counter +=1        \n",
    "                for i in range(faces_num):\n",
    "                    face_pos = lines[line_counter + 1 + i + 1].split()\n",
    "                    # [x1, y1, w, h]\n",
    "                    face_pos = face_pos[:4]\n",
    "                    face_pos = [int(i) for i in face_pos]\n",
    "                    faces_pos.append(face_pos)\n",
    "                real_img_path = osp.join(img_dir, img_path)\n",
    "                if osp.exists(real_img_path) and faces_num!=0:\n",
    "                    try:\n",
    "                        Image.open(real_img_path).verify()\n",
    "                        img_faces.append([real_img_path, faces_pos])\n",
    "                    except:\n",
    "                        print(\"*** warning:cannot open\",real_img_path)\n",
    "                else:\n",
    "                    print(\"*** warning:image path invalid\")\n",
    "                line_counter += (2 + faces_num)\n",
    "                \n",
    "##########################################用来控制选取多少图片来训练\n",
    "                if img_count>=5000:\n",
    "                    break\n",
    "        return img_faces\n",
    "    else:\n",
    "        print('*** warning:WILDER_FACE txt file not exist!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.对Landmark图片的预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def landmark_dataset_txt_parser(txt_path, img_dir):\n",
    "    \"\"\"\n",
    "    :param txt_path:\n",
    "    :param img_dir:\n",
    "    :return: [absolute_img_path,[x1,x2,y1,y2],(x,y)of[left_eye,right_eye,nose,mouse_left, mouse_right]]\n",
    "    \"\"\"\n",
    "    if txt_path is None or img_dir is None:\n",
    "        return []\n",
    "    if osp.exists(txt_path):\n",
    "        # *** img_faces shape :[img_path,[faces_num, 4]]\n",
    "        img_faces = []\n",
    "        with open(txt_path, 'r') as f:\n",
    "            l = []\n",
    "            lines = list(map(lambda line: line.strip().split('\\n'), f))\n",
    "            # lines[[str],[str],[]...]\n",
    "            lines = [i[0].split(' ') for i in lines]\n",
    "            # lines [[path_str,pos_str]...]\n",
    "            for line in lines:\n",
    "                # 将路径中的'\\'替换为'/'\n",
    "                img_path = line[0].replace('\\\\', '/')\n",
    "                faces_pos = [int(i) for i in line[1:5]]\n",
    "                # 标注为 左右眼，嘴，左右嘴角\n",
    "                landmark = [float(i) for i in line[5:]]\n",
    "                real_img_path = osp.join(img_dir, img_path)\n",
    "                # if DEBUG: print(real_img_path)\n",
    "                # if DEBUG: print(osp.exists(real_img_path), Image.open(real_img_path).verify())\n",
    "                if osp.exists(real_img_path):\n",
    "                    try:\n",
    "                        Image.open(real_img_path).verify()\n",
    "                        img_faces.append([real_img_path, faces_pos, landmark])\n",
    "                    except:\n",
    "                        print('Invalid image',real_img_path)\n",
    "                else:\n",
    "                    print(\"*** warning:image path invalid\")\n",
    "\n",
    "        # for i in img_faces: print(i)\n",
    "        return img_faces\n",
    "    else:\n",
    "        print('*** warning:WILDER_FACE txt file not exist!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 对demo的图片进行预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_widerdata(data, num_images, Random=True):\n",
    "    \"\"\"\n",
    "    data: train, valid\n",
    "    \"\"\"\n",
    "    if data==\"train\":\n",
    "        img_faces = create_pnet_data_txt_parser(args.class_traindata_txt_path, args.class_traindata_dir)\n",
    "        print(\"get train images\")\n",
    "    elif data==\"valid\":\n",
    "        img_faces = create_pnet_data_txt_parser(args.class_validdata_txt_path, args.class_validdata_dir)\n",
    "        print(\"get valid images\")\n",
    "        \n",
    "    total=len(img_faces)\n",
    "    result=[]\n",
    "    indexes=[]\n",
    "    while len(result)<num_images:\n",
    "        if Random: \n",
    "            index=random.randint(0,total-1)\n",
    "            if index not in indexes:\n",
    "                result.append(img_faces[index])\n",
    "        else:\n",
    "            for i in range(num_images):\n",
    "                result.append(img_faces[i])\n",
    "            \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# print 图片的functions："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 比较ground truth 和 pred_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imageshow(data,num_images,Random,net_name,save_name):\n",
    "    \"\"\"\n",
    "    :param data可以选 \"train\" \"valid\"\n",
    "    :param img_dir:\n",
    "    :return: [absolute_img_path,[x1,x2,y1,y2],(x,y)of[left_eye,right_eye,nose,mouse_left, mouse_right]]\n",
    "    \"\"\"\n",
    "    data_for_demo = get_widerdata(data, num_images,Random)\n",
    "    num_demo = len(data_for_demo)\n",
    "    fig, ax = plt.subplots(num_demo,2,dpi = 600, figsize=(2,num_demo))\n",
    "    pnet=load_net(args, \"pnet\")\n",
    "    for i in range(num_demo):\n",
    "        img_pth, gts = data_for_demo[i]\n",
    "        img = Image.open(img_pth)\n",
    "        bounding_boxes = pnet_boxes(img, pnet, min_face_size=MIN_FACE_SIZE, thresholds=THRESHOLDS, nms_thresholds=NMS_THRESHOLDS)\n",
    "        if net_name==\"rnet\":\n",
    "            rnet=load_net(args, net_name)\n",
    "            bounding_boxes =rnet_boxes(img, rnet, bounding_boxes, thresholds=THRESHOLDS, nms_thresholds=NMS_THRESHOLDS)\n",
    "        ax[i,0].imshow(img)\n",
    "        for b in gts:\n",
    "            ax[i,0].add_patch(plt.Rectangle((b[0], b[1]), b[2], b[3], fill=False, edgecolor='cyan',linewidth=0.4))\n",
    "\n",
    "        ax[i,1].imshow(img)\n",
    "        ax[i,1].text(0, 0, '# boxes: '+str(len(bounding_boxes)),fontsize=3)\n",
    "        # since there are too many boxes, only ten are drawn                \n",
    "####这里可以改变打出来的框的数量哦！\n",
    "        for b in bounding_boxes[:10]:\n",
    "            ax[i,1].add_patch(plt.Rectangle((b[0], b[1]), b[2]-b[0], b[3]-b[1], fill=False, edgecolor='cyan',linewidth=0.4))   \n",
    "    for axes in ax.ravel():\n",
    "        axes.axis('off')\n",
    "    ax[0][0].set_title(\"Ground truth\",fontsize=5)\n",
    "    ax[0][1].set_title(\"P-Net performance\",fontsize=5)\n",
    "    plt.tight_layout() #让图片紧密 \n",
    "    fig.show()\n",
    "    fig.savefig(save_name+'_demo.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. print Loss图像"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_show(net_name,train_loss,valid_loss):\n",
    "    # visualize the loss as the network trained\n",
    "    fig = plt.figure(figsize=(10,8))\n",
    "    plt.plot(range(1,len(train_loss)+1),train_loss, label='Training Loss')\n",
    "    plt.plot(range(1,len(valid_loss)+1),valid_loss,label='Validation Loss')\n",
    "    plt.xlabel('epochs',fontsize=30)\n",
    "    plt.ylabel('loss',fontsize=30)\n",
    "\n",
    "    #plt.ylim(0, 0.5) # consistent scale\n",
    "    plt.xlim(1, len(train_loss)+1) # consistent scale\n",
    "    plt.yticks(fontsize=30)\n",
    "    plt.xticks(fontsize=30)\n",
    "    plt.grid(True)\n",
    "    plt.legend(loc = 'best',fontsize=30)\n",
    "    plt.tight_layout()\n",
    "    fig.savefig('{}_loss.png'.format(net_name), bbox_inches='tight',dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 设定Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InplaceDataset(data.Dataset):\n",
    "    def __init__(self, img_face_landmark, img_faces, cropsize, pnet=None, rnet=None, ratio=(2, 1, 1, 1)):\n",
    "        \"\"\"\n",
    "        :param train_data_list: [train_data_num,[img_path,labels,[offsets],[landmark]]\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.img_faces = img_face_landmark + img_faces\n",
    "        shuffle(self.img_faces)\n",
    "        self.crop_size = cropsize\n",
    "        self.pnet = pnet\n",
    "        self.rnet = rnet\n",
    "        ratio_sum = float(sum(ratio))\n",
    "        self.ratio = [i / ratio_sum for i in ratio]\n",
    "        self.cache = []\n",
    "        #print('===> data set size:{}'.format(self.__len__()))\n",
    "        # self.dict = {'p': 0.0, 'pf': 0.0, 'l': 1.0, 'n': 0.0}\n",
    "\n",
    "    def get_img_faces_ldmk(self, index):\n",
    "        def load_img(img_path):\n",
    "            try:\n",
    "                img = Image.open(img_path)\n",
    "                img = img.convert('RGB')\n",
    "            except Exception:\n",
    "                print('*** warning loading fail!')\n",
    "                return\n",
    "            return img\n",
    "\n",
    "        img_face = self.img_faces[index]\n",
    "        img_path = img_face[0]\n",
    "        #这个图片一共标注了多少个face\n",
    "        faces = np.array(img_face[1])\n",
    "        \n",
    "        \n",
    "      \n",
    "        if faces.ndim is 1:\n",
    "            # img_face_landmark\n",
    "            # [absolute_img_path,[x1,x2,y1,y2],(x,y)of[left_eye,right_eye,nose,mouse_left, mouse_right]]\n",
    "            try:\n",
    "                faces = np.expand_dims(faces, 0)\n",
    "                faces[:, :] = faces[:, (0, 2, 1, 3)]\n",
    "            except:\n",
    "                print('error:',img_path)\n",
    "            \n",
    "\n",
    "        else:\n",
    "            # [img_num * [absolute_img_path, [faces_num * 4(which is x1, y1, w, h)]]]\n",
    "            faces[:, 2] += faces[:, 0]\n",
    "            faces[:, 3] += faces[:, 1]\n",
    "        # print('faces:{}'.format(faces))\n",
    "        ldmk = None if len(img_face) < 3 else [int(i) for i in img_face[2]]\n",
    "\n",
    "        return load_img(img_path), faces, ldmk\n",
    "\n",
    "    def get_crop_img_label_offset_ldmk(self, img, faces, ldmk, index):\n",
    "        def get_crop_img(img_np, crop_box, crop_size):\n",
    "            crop_box = [int(i) for i in crop_box]\n",
    "            crop_img_np = img_np[crop_box[1]:crop_box[3], crop_box[0]:crop_box[2], :]\n",
    "            crop_img = Image.fromarray(crop_img_np, mode='RGB')\n",
    "            crop_img = crop_img.resize((crop_size, crop_size), resample=PIL.Image.BILINEAR)\n",
    "            return crop_img\n",
    "\n",
    "        def get_real_label(label):\n",
    "            return {'n': 'n', 'np': 'n', 'pf': 'pf' if ldmk is None else 'l',\n",
    "                    'p': 'p' if ldmk is None else 'l'}.get(label)\n",
    "\n",
    "        def cal_offset(face, box):\n",
    "            if box is None:\n",
    "                return []\n",
    "            offset = [\n",
    "                (face[0] - box[0]) / float(box[2] - box[0]),\n",
    "                (face[1] - box[1]) / float(box[3] - box[1]),\n",
    "                (face[2] - box[2]) / float(box[2] - box[0]),\n",
    "                (face[3] - box[3]) / float(box[3] - box[1]),\n",
    "            ]\n",
    "            return offset\n",
    "\n",
    "        def cal_landmark_offset(box, ldmk):\n",
    "            if ldmk is None or box is None:\n",
    "                return []\n",
    "            else:\n",
    "                minx, miny = box[0], box[1]\n",
    "                w, h = box[2] - box[0], box[3] - box[1]\n",
    "                ldmk_offset = [(ldmk[i] - [minx, miny][i % 2]) / float([w, h][i % 2]) for i in range(len(ldmk))]\n",
    "                # print('box:{},ldmk:{},ldmk_offset:{}'.format(box, ldmk, ldmk_offset))\n",
    "                return ldmk_offset\n",
    "\n",
    "        img_np = np.array(img)\n",
    "        width, height = img.size\n",
    "        \n",
    "        if self.pnet is None:\n",
    "            # negative, negative partial, partial face, positive\n",
    "            label = np.random.choice(['n', 'np', 'pf', 'p'], p=self.ratio)\n",
    "            # label = 'np'\n",
    "            # print('label:{}'.format(label))\n",
    "            iou_th = {'n': (0, 0.3), 'np': (0, 0.3), 'pf': (0.4, 0.65), 'p': (0.65, 1.0)}.get(label)\n",
    "            sigma = {'n': 1, 'np': 0.3, 'pf': 0.1, 'p': 0.02}.get(label)\n",
    "            face, face_max_size = None, None\n",
    "            for i in range(10):\n",
    "                face = faces[random.randint(0,len(faces)-1)]\n",
    "                face_max_size = max(face[2] - face[0], face[3] - face[1])\n",
    "                if face_max_size > self.crop_size:\n",
    "                    break\n",
    "            crop_img = None\n",
    "            crop_box = None\n",
    "            for i in range(10):\n",
    "                # if ct >= sample_num: break\n",
    "                max_size = min(width, height)\n",
    "                size = (uniform(-1.0, 1.0) * sigma + 1) * face_max_size\n",
    "                # 保证大于剪切的尺寸要大于一个值\n",
    "                size = min(max(self.crop_size, size), max_size)\n",
    "                # print('size:', size)\n",
    "                x1, y1 = face[0], face[1]\n",
    "                crop_x1, crop_y1 = (uniform(-1.0, 1.0) * sigma + 1) * x1, (uniform(-1.0, 1.0) * sigma + 1) * y1\n",
    "                crop_x1, crop_y1 = min(max(0, crop_x1), width - size), min(max(0, crop_y1), height - size)\n",
    "                crop_box = np.array([int(crop_x1), int(crop_y1), int(crop_x1 + size), int(crop_y1 + size)])\n",
    "                # print('crop_box:', crop_box)\n",
    "                # print('faces_two_points:', faces_two_points)\n",
    "                iou = IoU(crop_box, np.array([face]))\n",
    "                iou_max_idx = iou.argmax()\n",
    "                iou = iou.max()\n",
    "                # print('iou', iou)\n",
    "                # iou值不符则跳过\n",
    "                if iou < iou_th[0] or iou > iou_th[1]:\n",
    "                    continue\n",
    "                else:\n",
    "                    # print('img_np:{}'.format(img_np))\n",
    "                    crop_img = get_crop_img(img_np, crop_box, self.crop_size)\n",
    "                    # crop_img.show()\n",
    "                    break\n",
    "            return crop_img, get_real_label(label), cal_offset(face, crop_box), cal_landmark_offset(crop_box, ldmk)\n",
    "        else:\n",
    "            # negative, negative partial, partial face, positive\n",
    "            if len(self.cache) != 0:\n",
    "                self.img_faces.append(self.img_faces[index])\n",
    "                return self.cache.pop(0)\n",
    "            iou_th = {'n': (0, 0.3), 'pf': (0.4, 0.65), 'p': (0.65, 1.0)}\n",
    "            # sigma = {'n': 1, 'np': 0.3, 'pf': 0.1, 'p': 0.02}\n",
    "            bounding_boxes = pnet_boxes(img, self.pnet, show_boxes=False)\n",
    "            if bounding_boxes is None:\n",
    "                return None, None, None, None\n",
    "            if self.rnet is not None:\n",
    "                bounding_boxes_rnet = rnet_boxes(img, self.rnet, bounding_boxes, show_boxes=False)\n",
    "                if len(bounding_boxes_rnet) != 0:\n",
    "                    bounding_boxes = np.vstack((bounding_boxes, bounding_boxes_rnet))\n",
    "            crop_img = None\n",
    "            crop_box = None\n",
    "            closet_face = None\n",
    "            for id, box in enumerate(bounding_boxes, start=1):\n",
    "                box = [min(max(0, int(box[i])), width if i % 2 == 0 else height) for i in range(4)]\n",
    "                if box[2] - box[0] < self.crop_size: continue\n",
    "                iou = IoU(box, faces)\n",
    "                iou_max = iou.max()\n",
    "                iou_index = iou.argmax()\n",
    "                closet_face = faces[iou_index]\n",
    "                # print('iou_max:{}, iou_index:{}'.format(iou_max, iou_index))\n",
    "                # ioumax = max(iou, iou_max)\n",
    "                crop_img = get_crop_img(img_np=img_np, crop_box=box, crop_size=self.crop_size)\n",
    "                # img_box.show()\n",
    "                # [(0, 0.3), (0.4, 0.65), (0.65, 1.0)]\n",
    "                for temp_label in iou_th:\n",
    "                    if iou_max < iou_th[temp_label][0] or iou_max > iou_th[temp_label][1]:\n",
    "                        continue\n",
    "                    else:\n",
    "                        label = temp_label\n",
    "                        crop_box = box\n",
    "                        crop_img = get_crop_img(img_np, box, self.crop_size)\n",
    "                        self.cache.append((crop_img, get_real_label(label),\n",
    "                                           cal_offset(closet_face, crop_box), cal_landmark_offset(crop_box, ldmk)))\n",
    "\n",
    "            return (None, None, None, None) if len(self.cache) == 0 else self.cache.pop(0)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img, faces, ldmk = self.get_img_faces_ldmk(index)\n",
    "        crop_img, label, offset, ldmk = self.get_crop_img_label_offset_ldmk(img, faces, ldmk, index)\n",
    "        if crop_img is None: return self.__getitem__(random.randint(0, self.__len__()-1))\n",
    "        img_tensor = transforms.ToTensor()(crop_img)\n",
    "        landmark_flag = torch.FloatTensor([1.0 if label == 'l' else 0.0])\n",
    "        label = torch.FloatTensor([1.0 if label in ['p', 'pf', 'l'] else 0.0])\n",
    "        offset = torch.FloatTensor(offset if 4 == len(offset) else 4 * [0.0])\n",
    "        landmark = torch.FloatTensor(ldmk if 10 == len(ldmk) else 10 * [0.0])\n",
    "        return (img_tensor, label, offset, landmark_flag, landmark)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_faces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 设立dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 为了区分train data和valid data，此处引入了data这个参数来区分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inplace_data_loader(args, net_name,data=\"train\"):\n",
    "    if data==\"train\":\n",
    "        img_faces = create_pnet_data_txt_parser(args.class_traindata_txt_path, args.class_traindata_dir)\n",
    "        img_face_landmark = landmark_dataset_txt_parser(args.landmark_traindata_txt_path, args.landmark_traindata_dir)\n",
    "        if net_name == 'pnet':\n",
    "            IDS = InplaceDataset(img_face_landmark, img_faces, cropsize=12)\n",
    "        elif net_name == 'rnet':\n",
    "            IDS = InplaceDataset(img_face_landmark, img_faces, cropsize=24, pnet=load_net(args, 'pnet'))\n",
    "        elif net_name == 'onet':\n",
    "            IDS = InplaceDataset(img_face_landmark, img_faces, cropsize=48,\n",
    "                                 pnet=load_net(args, 'pnet'), rnet=load_net(args, 'rnet'))\n",
    "    elif data==\"valid\":\n",
    "        img_faces = create_pnet_data_txt_parser(args.class_validdata_txt_path, args.class_validdata_dir)\n",
    "        img_face_landmark = landmark_dataset_txt_parser(args.landmark_validdata_txt_path, args.landmark_validdata_dir)\n",
    "        if net_name == 'pnet':\n",
    "            IDS = InplaceDataset(img_face_landmark, img_faces, cropsize=12)\n",
    "        elif net_name == 'rnet':\n",
    "            IDS = InplaceDataset(img_face_landmark, img_faces, cropsize=24, pnet=load_net(args, 'pnet'))\n",
    "        elif net_name == 'onet':\n",
    "            IDS = InplaceDataset(img_face_landmark, img_faces, cropsize=48,\n",
    "                                 pnet=load_net(args, 'pnet'), rnet=load_net(args, 'rnet'))\n",
    "    return DataLoader(IDS,\n",
    "                      batch_size=args.batch_size,\n",
    "                      shuffle=True,\n",
    "                      num_workers=args.num_workers,\n",
    "                      pin_memory=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 核心model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class P_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(P_Net, self).__init__()\n",
    "        self.pre_layer = nn.Sequential(\n",
    "            # 12x12x3\n",
    "            nn.Conv2d(3, 10, kernel_size=3, stride=1),  # conv1\n",
    "            nn.PReLU(),  # PReLU1\n",
    "            # 10x10x10\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # pool1\n",
    "            # 5x5x10\n",
    "            nn.Conv2d(10, 16, kernel_size=3, stride=1),  # conv2\n",
    "            # 3x3x16\n",
    "            nn.PReLU(),  # PReLU2\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1),  # conv3\n",
    "            # 1x1x32\n",
    "            nn.PReLU()  # PReLU3\n",
    "        )\n",
    "        # detection\n",
    "        self.conv4_1 = nn.Conv2d(32, 1, kernel_size=1, stride=1)\n",
    "        # bounding box regresion\n",
    "        self.conv4_2 = nn.Conv2d(32, 4, kernel_size=1, stride=1)\n",
    "        # landmark localization\n",
    "        self.conv4_3 = nn.Conv2d(32, 10, kernel_size=1, stride=1)\n",
    "        # weight initiation with xavier\n",
    "        self.apply(weights_init)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pre_layer(x)\n",
    "        det = torch.sigmoid(self.conv4_1(x))\n",
    "        box = self.conv4_2(x)\n",
    "        landmark = self.conv4_3(x)\n",
    "        # det:[,2,1,1], box:[,4,1,1], landmark:[,10,1,1]\n",
    "        return det, box, landmark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class R_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(R_Net, self).__init__()\n",
    "        self.pre_layer = nn.Sequential(\n",
    "            # 24x24x3\n",
    "            nn.Conv2d(3, 28, kernel_size=3, stride=1),  # conv1\n",
    "            nn.PReLU(),  # prelu1\n",
    "            # 22x22x28\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),  # pool1\n",
    "            # 10x10x28\n",
    "            nn.Conv2d(28, 48, kernel_size=3, stride=1),  # conv2\n",
    "            nn.PReLU(),  # prelu2\n",
    "            # 8x8x48\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),  # pool2\n",
    "            # 3x3x48\n",
    "            nn.Conv2d(48, 64, kernel_size=2, stride=1),  # conv3\n",
    "            # 2x2x64\n",
    "            nn.PReLU()  # prelu3\n",
    "        )\n",
    "        # 2x2x64\n",
    "        self.conv4 = nn.Linear(64 * 2 * 2, 128)  # conv4\n",
    "        # 128\n",
    "        self.prelu4 = nn.PReLU()  # prelu4\n",
    "        # detection\n",
    "        self.conv5_1 = nn.Linear(128, 1)\n",
    "        # bounding box regression\n",
    "        self.conv5_2 = nn.Linear(128, 4)\n",
    "        # lanbmark localization\n",
    "        self.conv5_3 = nn.Linear(128, 10)\n",
    "        # weight initiation weih xavier\n",
    "        self.apply(weights_init)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pre_layer(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.conv4(x)\n",
    "        x = self.prelu4(x)\n",
    "        det = torch.sigmoid(self.conv5_1(x))\n",
    "        #det = F.softmax(self.conv5_1(x), dim=1)#也可以试试softmax呀\n",
    "        box = self.conv5_2(x)\n",
    "        landmark = self.conv5_3(x)\n",
    "        return det, box, landmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## O-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class O_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(O_Net, self).__init__()\n",
    "        self.pre_layer = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=1),  # conv1\n",
    "            nn.PReLU(),  # prelu1\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),  # pool1\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1),  # conv2\n",
    "            nn.PReLU(),  # prelu2\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),  # pool2\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),  # conv3\n",
    "            nn.PReLU(),  # prelu3\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # pool3\n",
    "            nn.Conv2d(64, 128, kernel_size=2, stride=1),  # conv4\n",
    "            nn.PReLU()  # prelu4\n",
    "        )\n",
    "        self.conv5 = nn.Linear(128 * 2 * 2, 256)  # conv5\n",
    "        self.prelu5 = nn.PReLU()  # prelu5\n",
    "        # detection\n",
    "        self.conv6_1 = nn.Linear(256, 1)\n",
    "        # bounding box regression\n",
    "        self.conv6_2 = nn.Linear(256, 4)\n",
    "        # lanbmark localization\n",
    "        self.conv6_3 = nn.Linear(256, 10)\n",
    "        # weight initiation weih xavier\n",
    "        self.apply(weights_init)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pre_layer(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.conv5(x)\n",
    "        x = self.prelu5(x)\n",
    "        # detection\n",
    "        det = torch.sigmoid(self.conv6_1(x))\n",
    "        box = self.conv6_2(x)\n",
    "        landmark = self.conv6_3(x)\n",
    "        return det, box, landmark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用的loss-MTCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossFn:\n",
    "    def __init__(self, cls_factor=1, box_factor=1, landmark_factor=1):\n",
    "        # loss function\n",
    "        self.cls_factor = cls_factor\n",
    "        self.box_factor = box_factor\n",
    "        self.land_factor = landmark_factor\n",
    "        self.loss_cls = nn.BCELoss()\n",
    "        # binary cross entropy\n",
    "        self.loss_box = nn.MSELoss()\n",
    "        # mean square error\n",
    "        self.loss_landmark = nn.MSELoss()\n",
    "\n",
    "\n",
    "    def cls_loss(self,gt_label,pred_label):\n",
    "        pred_label = torch.squeeze(pred_label)\n",
    "        gt_label = torch.squeeze(gt_label)\n",
    "        # get the mask element which >= 0, only 0 and 1 can effect the detection loss\n",
    "        mask = torch.ge(gt_label,0)\n",
    "        valid_gt_label = torch.masked_select(gt_label,mask)\n",
    "        valid_pred_label = torch.masked_select(pred_label,mask)\n",
    "        return self.loss_cls(valid_pred_label,valid_gt_label)*self.cls_factor\n",
    "\n",
    "\n",
    "    def box_loss(self, gt_label, gt_offset, pred_offset):\n",
    "        # if gt_label is torch.tensor([0.0]):\n",
    "        #     return torch.tensor([0.0])\n",
    "        # pred_offset: [batch_size, 4] to [batch_size,4]\n",
    "        pred_offset = torch.squeeze(pred_offset)\n",
    "        # gt_offset: [batch_size, 4, 1, 1] to [batch_size,4]\n",
    "        gt_offset = torch.squeeze(gt_offset)\n",
    "        # gt_label: [batch_size, 1, 1, 1] to [batch_size]\n",
    "        gt_label = torch.squeeze(gt_label)\n",
    "\n",
    "        # get the mask element which != 0\n",
    "        # unmask = torch.eq(gt_label, 0)\n",
    "        # mask = torch.eq(unmask, 0)\n",
    "        mask = torch.eq(gt_label, 1)\n",
    "        # convert mask to dim index\n",
    "        \n",
    "        valid_gt_offset = gt_offset[mask, :]\n",
    "        valid_pred_offset = pred_offset[mask, :]\n",
    "        # print('valid_gt_offset', valid_gt_offset, 'valid_pred_offset', valid_pred_offset)\n",
    "        valid_sample_num = valid_gt_offset.shape[0]\n",
    "        if 0 == valid_sample_num:\n",
    "            # print('No box')\n",
    "            # return self.loss_box(torch.tensor([0.0]), torch.tensor([0.0]))\n",
    "            return torch.tensor([0.0])\n",
    "        else:\n",
    "            # print('valid_sample_num', valid_sample_num)\n",
    "            return self.loss_box(valid_pred_offset, valid_gt_offset) * self.box_factor\n",
    "        # return torch.tensor([0.])\n",
    "\n",
    "    def landmark_loss(self, landmark_flag, gt_landmark=None, pred_landmark=None):\n",
    "        # pred_landmark:[batch_size,10,1,1] to [batch_size,10]\n",
    "        pred_landmark = torch.squeeze(pred_landmark)\n",
    "        # gt_landmark:[batch_size,10] to [batch_size,10]\n",
    "        gt_landmark = torch.squeeze(gt_landmark)\n",
    "        # gt_label:[batch_size,1] to [batch_size]\n",
    "        gt_label = torch.squeeze(landmark_flag)\n",
    "        mask = torch.eq(gt_label, 1)\n",
    "        valid_gt_landmark = gt_landmark[mask, :]\n",
    "        valid_pred_landmark = pred_landmark[mask, :]\n",
    "        valid_sample_num = valid_gt_landmark.shape[0]\n",
    "        if 0 == valid_sample_num:\n",
    "            return torch.tensor([0.0])\n",
    "        else:\n",
    "            return self.loss_landmark(valid_pred_landmark, valid_gt_landmark) * self.land_factor\n",
    "\n",
    "    def total_loss(self, gt_label, pred_label, gt_offset, pred_offset, landmark_flag, gt_landmark, pred_landmark):\n",
    "        return self.cls_loss(gt_label, pred_label) \\\n",
    "               + self.box_loss(gt_label, gt_offset, pred_offset) \\\n",
    "               + self.landmark_loss(landmark_flag, gt_landmark, pred_landmark)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train 部分的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # 1. 根据网络层的不同定义不同的初始化方式     \n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_normal_(m.weight)\n",
    "        #nn.init.constant_(m.bias, 0) bias不要全初始化为0\n",
    "        nn.init.normal_(m.bias, mean=0, std=1)\n",
    "    # 也可以判断是否为conv2d，使用相应的初始化方式 \n",
    "    elif isinstance(m, nn.Conv2d):\n",
    "        nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "     # 是否为批归一化层\n",
    "    elif isinstance(m, nn.BatchNorm2d):\n",
    "        nn.init.constant_(m.weight, 1)\n",
    "        nn.init.constant_(m.bias, 0)\n",
    "#使用这样的初始化后，模型的初始表现确实好了些"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_net(args, net_name):\n",
    "    # pnet, rnet, onet = P_Net(), R_Net(), O_Net()\n",
    "    net_list = {'pnet': P_Net(), 'rnet': R_Net(), 'onet': O_Net()}\n",
    "    try:\n",
    "        net = net_list[net_name].to(device)\n",
    "        try:\n",
    "            print('===> loading the saved net weights...')\n",
    "            _ = osp.join(args.save_folder, net_name + '.pkl')\n",
    "            print('===> check {} saved path({}):{}'.format(net_name, _, osp.exists(_)))\n",
    "            net.load_state_dict(torch.load(_, map_location=device))\n",
    "            return net  # , rnet, onet\n",
    "        except Exception:\n",
    "            print('*** fail to load the saved net weights!')\n",
    "            return net\n",
    "    except Exception:\n",
    "        print('*** Net name wrong!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_para(file_name):\n",
    "    # para = None\n",
    "    try:\n",
    "        print('===> loading the saved parameters...')\n",
    "        para = torch.load(osp.join(args.save_folder, file_name))\n",
    "    except Exception:\n",
    "        print('*** fail to load the saved parameters!')\n",
    "        print('===> initailizing the parameters...')\n",
    "        para = {\n",
    "            'lr': args.lr,\n",
    "            'iter': 0,\n",
    "            'loss': [],\n",
    "            'val_result': [],\n",
    "            'optimizer_param': None\n",
    "        }\n",
    "        save_safely(para, dir_path=args.save_folder, file_name=file_name)\n",
    "    return para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_safely(file, dir_path, file_name):\n",
    "    if not osp.exists(dir_path):\n",
    "        os.mkdir(dir_path)\n",
    "        print('*** dir not exist, created one')\n",
    "    save_path = osp.join(dir_path, file_name)\n",
    "    if osp.exists(save_path):\n",
    "        temp_name = save_path + '.temp'\n",
    "        torch.save(file, temp_name)\n",
    "        os.remove(save_path)\n",
    "        os.rename(temp_name, save_path)\n",
    "        #print('*** find the file conflict while saving, saved safely')\n",
    "    else:\n",
    "        torch.save(file, save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_net(args, net_name='pnet',loss_config=[]):\n",
    "    net = load_net(args, net_name)\n",
    "    para = load_para(net_name + '_para.pkl')\n",
    "    lr = para['lr']\n",
    "    iter_count = para['iter']\n",
    "    optimizer = opt.Adam(net.parameters(), lr=args.lr, betas=(0.9, 0.999), eps=1e-08, amsgrad=True)\n",
    "    loss = LossFn(cls_factor=loss_config[0], box_factor=loss_config[1], landmark_factor=loss_config[2])\n",
    "    if para['optimizer_param'] is not None:\n",
    "        optimizer.state_dict()['param_groups'][0].update(para['optimizer_param'])\n",
    "        print('===> updated the param of optimizer.')\n",
    "    #导入dataloader\n",
    "    train_loader = get_inplace_data_loader(args, net_name,data=\"train\")\n",
    "    valid_loader = get_inplace_data_loader(args, net_name,data=\"valid\")\n",
    "    # to track the training loss as the model trains\n",
    "    train_losses = []\n",
    "    # to track the validation loss as the model trains\n",
    "    valid_losses = []\n",
    "    # to track the average training loss per epoch as the model trains\n",
    "    avg_train_losses = []\n",
    "    # to track the average validation loss per epoch as the model trains\n",
    "    avg_valid_losses = [] \n",
    "    \n",
    "    for i in range(args.epoch):\n",
    "        print('Epoch {}/{}'.format(i+1, args.epoch))\n",
    "        print('-' * 10)\n",
    "        t0 = time.perf_counter()\n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        net.train()\n",
    "        for step, (img_tensor, label, offset, landmark_flag, landmark) in enumerate(train_loader):\n",
    "            iter_count += 1\n",
    "            # update lr rate\n",
    "            wrap = (img_tensor, label, offset, landmark)\n",
    "            (img_tensor, label, offset, landmark) = [i.to(device) for i in wrap]\n",
    "            det, box, ldmk = net(img_tensor)\n",
    "            optimizer.zero_grad()\n",
    "            # print('offset:', offset)\n",
    "            all_loss = loss.total_loss(gt_label=label, pred_label=det, gt_offset=offset, pred_offset=box,\n",
    "                                       landmark_flag=landmark_flag, pred_landmark=ldmk, gt_landmark=landmark)\n",
    "            \n",
    "            all_loss.backward()\n",
    "            optimizer.step()\n",
    "            train_losses.append(all_loss.item())\n",
    "            # 判断是否保存参数\n",
    "            if 0 == iter_count % args.save_steps:\n",
    "                if 0 == iter_count % args.half_lr_steps:\n",
    "                    lr /= 2\n",
    "                    para.update({'lr': lr})\n",
    "                    for param_groups in optimizer.param_groups:\n",
    "                        param_groups['lr'] = lr\n",
    "                    print('*** lr updated:{}'.format(lr))\n",
    "                para.update({\n",
    "                    'lr': lr,\n",
    "                    'iter': iter_count,\n",
    "                    'optimizer_param': optimizer.state_dict()['param_groups'][0]\n",
    "                })\n",
    "                save_safely(net.state_dict(), args.save_folder, net_name + '.pkl')\n",
    "                save_safely(para, args.save_folder, net_name + '_para.pkl')    \n",
    "        ######################    \n",
    "        # validate the model #\n",
    "        ######################\n",
    "        net.eval()\n",
    "        for step, (img_tensor, label, offset, landmark_flag, landmark) in enumerate(valid_loader):\n",
    "            # update lr rate\n",
    "            wrap = (img_tensor, label, offset, landmark)\n",
    "            (img_tensor, label, offset, landmark) = [i.to(device) for i in wrap]\n",
    "            det, box, ldmk = net(img_tensor)\n",
    "            optimizer.zero_grad()\n",
    "            # print('offset:', offset)\n",
    "            all_loss = loss.total_loss(gt_label=label, pred_label=det, gt_offset=offset, pred_offset=box,\n",
    "                                       landmark_flag=landmark_flag, pred_landmark=ldmk, gt_landmark=landmark)\n",
    "            valid_losses.append(all_loss.item())\n",
    "            t1 = time.perf_counter()\n",
    "            \n",
    "            \n",
    "        # print training/validation statistics \n",
    "        # calculate average loss over an epoch\n",
    "        train_loss = np.average(train_losses)\n",
    "        valid_loss = np.average(valid_losses)\n",
    "        avg_train_losses.append(train_loss)\n",
    "        avg_valid_losses.append(valid_loss)\n",
    "        print_msg = (f'train_loss: {train_loss:.5f} '+f'|   valid_loss: {valid_loss:.5f} ')\n",
    "        print(print_msg)\n",
    "        # clear lists to track next epoch\n",
    "        train_losses = []\n",
    "        valid_losses = []   \n",
    "        #print这一epoch的时间和lr\n",
    "        t1 = time.perf_counter()\n",
    "        print('===> iter:{}\\t|lr:{:.12f} | time:{:.8f}'.format(iter_count, lr, t1 - t0))\n",
    "    return avg_train_losses, avg_valid_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 开始训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.训练P-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss, valid_loss=train_net(args, net_name='pnet',loss_config=net_loss_config['pnet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_show(\"pnet\",train_loss,valid_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imageshow(data=\"train\",num_images=8,Random=False,net_name='pnet',save_name=\"pnet-try-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.训练R-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss, valid_loss=train_net(args, net_name='rnet',loss_config=net_loss_config['rnet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_show(\"rnet\",train_loss,valid_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imageshow(data=\"train\",num_images=8,Random=False,net_name='rnet',save_name=\"rnet-try-2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练O-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_net(args, net_name='onet',loss_config=net_loss_config['onet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_show(\"onet\",train_loss,valid_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imageshow(data=\"train\",num_images=8,Random=False,net_name='onet',save_name=\"onet-try-2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一些方程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onet_test(args, img_path):\n",
    "    img = load_img(img_path)\n",
    "    net = load_net(args, 'pnet')\n",
    "    output = net((transforms.ToTensor()(img.resize((12, 12), Image.BILINEAR)).to(device)).unsqueeze(0))\n",
    "    print('prob:', output)\n",
    "    return show_bboxes(img, [[(250 * t.item() + 250 * (i > 1)) for i, t in enumerate(output[1][0])]])\n",
    " \n",
    "def img_face_detect(args, img_path, th=[0.6, 0.7, 0.8]):\n",
    "    img = None\n",
    "    try:\n",
    "        print('===> loading the img...')\n",
    "        img = Image.open(img_path)\n",
    "        img = img.convert('RGB')\n",
    "    except Exception:\n",
    "        print('*** warning loading fail!')\n",
    "        return\n",
    "    img_tensor = transforms.ToTensor()(img).unsqueeze(0).to(device)\n",
    "    pnet, rnet, onet = load_net(args, 'pnet'), load_net(args, 'rnet'), load_net(args, 'onet')\n",
    "    resize_ratio = 0.7071\n",
    "    det, box, _ = pnet(img_tensor)\n",
    "    det_faces = det.ge(th[0])\n",
    "    print(det)\n",
    "    return det_faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path=\"../input/wider-face-recognization/WIDER_train/WIDER_train/images/0--Parade/0_Parade_Parade_0_11.jpg\"\n",
    "amm=img_face_detect(args, img_path, th=[0.6, 0.7, 0.8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(amm[0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_img(img_path):\n",
    "    try:\n",
    "        print('===> loading the img...')\n",
    "        img = Image.open(img_path)\n",
    "        img = img.convert('RGB')\n",
    "    except Exception:\n",
    "        print('*** warning loading fail!')\n",
    "        return\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 开始检测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=onet_test(args, '../input/wider-face-recognization/WIDER_train/WIDER_train/images/0--Parade/0_Parade_Parade_0_127.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.dpi = 600\n",
    "plt.imshow(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Test():\n",
    "    def __init__(self, photo, net):\n",
    "        self.photo = photo\n",
    "        self.img, self.img_data, self.img_x, self.img_y = self.get_img_data(photo)\n",
    "\n",
    "        self.model = net\n",
    "        print(self.model)\n",
    "        self.model = torch.load(r'C:\\Users\\Administrator\\Desktop\\myproject\\MTCNN\\log_P_trian')\n",
    "        # self.model.eval()\n",
    "\n",
    "        self.get_net_out()\n",
    "\n",
    "    def get_img_data(self, image):  # 1. 獲取圖片資料\n",
    "        img = 0\n",
    "        if type(image) == str and os.path.exists(image) == True:\n",
    "            img = Image.open(image)\n",
    "        else:\n",
    "            img = image\n",
    "\n",
    "        img_x, img_y = img.size\n",
    "        # 將圖片轉成陣列形式\n",
    "        img_data = torch.Tensor(np.array(img))\n",
    "        # 2.對資料進行處理\n",
    "        img_data = img_data / 255 - 0.5\n",
    "        img_data = img_data.unsqueeze(0)  # 在原有維度 的第一維度升維\n",
    "\n",
    "        return img, img_data.permute(0, 3, 1, 2), img_x, img_y\n",
    "\n",
    "    # 3. 獲取 P net 輸出\n",
    "    def get_net_out(self):\n",
    "\n",
    "        have_face = []  # 儲存網路輸出值\n",
    "        box_face = []\n",
    "        count = 1\n",
    "        while True:  # 影象金字塔\n",
    "            if self.img_size(self.img_x, self.img_y) == True:\n",
    "\n",
    "                face_out, offset = self.model(self.img_data)  # 獲取P-net輸出\n",
    "                # face_out.size() [1, 2, 145, 295]\n",
    "                # offset.size() [1, 4, 145, 295]\n",
    "\n",
    "                box_offset = self.add_index(offset)\n",
    "                # 改變形狀：[1, 2, 145, 295] ==> [42775, 2]\n",
    "                face_out = face_out.view(-1, face_out.size(1))  # torch.Size([42775, 2])\n",
    "                # 改變形狀：[ 145, 295,6]==> [42775, 6]\n",
    "                offset = box_offset.view(-1, box_offset.size(2))  # torch.Size([42775, 4])\n",
    "                # 獲取最大值索引\n",
    "                face_out = torch.argmax(face_out, 1)\n",
    "                # 儲存輸出\n",
    "                have_face.extend(face_out.detach().numpy())\n",
    "                box_face.extend(offset.detach().numpy())\n",
    "                # print(count, face_out.detach().numpy().shape, offset.detach().numpy().shape)\n",
    "                count += 1  # 將圖片縮小一次 +1\n",
    "                # 影象縮小0.7倍\n",
    "                self.img_x = int(self.img_x * 0.7)\n",
    "                self.img_y = int(self.img_y * 0.7)\n",
    "                img2 = self.img.resize((self.img_x, self.img_y))\n",
    "                # 獲取圖片資料\n",
    "                self.img_data = self.get_img_data(img2)\n",
    "                self.img_data = self.img_data[1]\n",
    "                print(np.array(have_face).shape)\n",
    "\n",
    "                one = torch.ne(torch.Tensor(have_face), 0)  # 獲取非0索引\n",
    "                print(one)\n",
    "                have_face_box = torch.Tensor(box_face)[one]  # 獲取有人臉的偏移量\n",
    "                self.filter(have_face_box)\n",
    "            else:\n",
    "                break\n",
    "        self.img.show()\n",
    "\n",
    "    # 4.篩選重複的"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
