{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 人脸识别"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考代码（主体框架）https://github.com/GitHberChen/MTCNN_Pytorch\n",
    "\n",
    "参考代码（Onet最后训练的显示部分）https://github.com/zhangjiahao1026/MTCNN-Pytorch\n",
    "\n",
    "查阅约20篇关于MTCNN的实现文章，最终选取第一个的代码加以改编。\n",
    "\n",
    "优化：\n",
    "\n",
    "1. 进一步整合代码。\n",
    "\n",
    "2. imageshow增加了o-net。\n",
    "\n",
    "3. landmark的加入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#基本的引入，按字母表排序\n",
    "import argparse\n",
    "import cv2\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Circle\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "from numpy.random import uniform\n",
    "import os\n",
    "from os import path as osp\n",
    "import PIL\n",
    "from PIL import Image, ImageDraw\n",
    "import random\n",
    "from random import shuffle\n",
    "import time\n",
    "#torch部分的import\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as opt\n",
    "#from tensorboardX import SummaryWriter\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "#warning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 设置随机数种子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "# 设置随机数种子\n",
    "setup_seed(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 设置args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 原文作者通过arg减少了很多重复的语句，而且在args上可以很快的更改一些数据。其便利性让我沿用这个写法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "global device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if torch.cuda.is_available():\n",
    "    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "    cudnn.benchmark = True\n",
    "    cudnn.deterministic = True\n",
    "else:\n",
    "    torch.set_default_tensor_type('torch.FloatTensor')\n",
    "    # torch.set_default_tensor_type('torch.DoubleTensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def config():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    #training_data的路径设置\n",
    "    parser.add_argument('--class_traindata_txt_path',\n",
    "                        default='wider_face_split/wider_face_train_bbx_gt.txt',\n",
    "                        type=str, help='the path of wider_face_train_bbx_gt.txt')\n",
    "    parser.add_argument('--class_traindata_dir', default='WIDER_train/images',\n",
    "                        type=str, help='the dir of WILDER FACE train image file')\n",
    "    parser.add_argument('--landmark_traindata_txt_path',\n",
    "                        default=\"CNN/trainImageList.txt\", type=str, help='the path of CelebA .txt file')\n",
    "    parser.add_argument('--landmark_traindata_dir', \n",
    "                        default=\"CNN\", type=str,help='the dir of CelebA image file')\n",
    "   \n",
    "    #valid_data的路径设置\n",
    "    parser.add_argument('--class_validdata_txt_path',\n",
    "                        default='wider_face_split/wider_face_val_bbx_gt.txt',\n",
    "                        type=str, help='the path of wider_face_train_bbx_gt.txt')\n",
    "    parser.add_argument('--class_validdata_dir', default='WIDER_val/images',\n",
    "                        type=str, help='the dir of WILDER FACE valid image file')\n",
    "    \n",
    "    parser.add_argument('--landmark_validdata_txt_path',\n",
    "                        default=\"CNN/testImageList.txt\", type=str, help='the path of CelebA .txt file')\n",
    "    parser.add_argument('--landmark_validdata_dir', \n",
    "                        default=\"CNN\", type=str,help='the dir of CelebA image file')\n",
    "    #数据增强设置\n",
    "    parser.add_argument('--class_data_augment', default=3,\n",
    "                        type=int, help='the augment ratio for create pnet data set')\n",
    "    #储存路径设置\n",
    "    parser.add_argument('--save_folder', type=str,default='CHUHR2',  \n",
    "                        help='the folder of p/r/onet_para.pkl, p/r/onet.pkl saved')\n",
    "    parser.add_argument('--train_net', type=str,\n",
    "                        default='pnet', choices=['pnet', 'rnet', 'onet'],\n",
    "                        help='choose net to train')\n",
    "    \n",
    "    #超参数设置\n",
    "    parser.add_argument('--lr', type=float,\n",
    "                        default=0.001,\n",
    "                        help='initial learning rate')\n",
    "    parser.add_argument('--epoch', type=int,\n",
    "                        default=40,\n",
    "                        help='some batches make up a sub_epoch ')\n",
    "    parser.add_argument('--batch_size', type=int,\n",
    "                        default=32,\n",
    "                        help='batch_size ')\n",
    "    #这里的num——worker数量如果不为零，且使用GPU，是会报错的。\n",
    "    parser.add_argument('--num_workers', type=int,\n",
    "                        default=0,\n",
    "                        help='workers for loading the data')\n",
    "    parser.add_argument('--half_lr_steps', type=int,\n",
    "                        default=10000,\n",
    "                        help='half the lr every half_lr_steps iter')\n",
    "    parser.add_argument('--save_steps', type=int,\n",
    "                        default=10,\n",
    "                        help='save para, model every save_steps iter')\n",
    "    \n",
    "    args = parser.parse_args(args=[])\n",
    "\n",
    "    return args\n",
    "\n",
    "args=config()\n",
    "#后面的Loss的内置系数\n",
    "net_loss_config = {\n",
    "    'pnet': [1.0, 0.5, 0.5],\n",
    "    'rnet': [1.0, 0.5, 0.5],\n",
    "    'onet': [1.0, 0.5, 1.0]\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nms(boxes, overlap_threshold=0.5, mode='union'):\n",
    "    \"\"\" Pure Python NMS baseline. \"\"\"\n",
    "    x1 = boxes[:, 0]\n",
    "    y1 = boxes[:, 1]\n",
    "    x2 = boxes[:, 2]\n",
    "    y2 = boxes[:, 3]\n",
    "    scores = boxes[:, 4]\n",
    "\n",
    "    areas = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
    "    # argsort()默认从小到大排序，取反后就是从大到小\n",
    "    order = scores.argsort()[::-1]\n",
    "\n",
    "    keep = []\n",
    "    while order.size > 0:\n",
    "        i = order[0]\n",
    "        keep.append(i)\n",
    "        xx1 = np.maximum(x1[i], x1[order[1:]])\n",
    "        yy1 = np.maximum(y1[i], y1[order[1:]])\n",
    "        xx2 = np.minimum(x2[i], x2[order[1:]])\n",
    "        yy2 = np.minimum(y2[i], y2[order[1:]])\n",
    "\n",
    "        w = np.maximum(0.0, xx2 - xx1 + 1)\n",
    "        h = np.maximum(0.0, yy2 - yy1 + 1)\n",
    "        inter = w * h\n",
    "\n",
    "        if mode is 'min':\n",
    "            ovr = inter / np.minimum(areas[i], areas[order[1:]])\n",
    "        else:\n",
    "            ovr = inter / (areas[i] + areas[order[1:]] - inter)\n",
    "\n",
    "        inds = np.where(ovr <= overlap_threshold)[0]\n",
    "        order = order[inds + 1]\n",
    "    # print(keep)\n",
    "    return keep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_square(bboxes):\n",
    "    \"\"\"\n",
    "    Convert bounding boxes to a square form.\n",
    "    \"\"\"\n",
    "    # 将矩形对称扩大为正方形\n",
    "    square_bboxes = np.zeros_like(bboxes)\n",
    "    x1, y1, x2, y2 = [bboxes[:, i] for i in range(4)]\n",
    "    h = y2 - y1 + 1.0\n",
    "    w = x2 - x1 + 1.0\n",
    "    max_side = np.maximum(h, w)\n",
    "    square_bboxes[:, 0] = x1 + w * 0.5 - max_side * 0.5\n",
    "    square_bboxes[:, 1] = y1 + h * 0.5 - max_side * 0.5\n",
    "    square_bboxes[:, 2] = square_bboxes[:, 0] + max_side - 1.0\n",
    "    square_bboxes[:, 3] = square_bboxes[:, 1] + max_side - 1.0\n",
    "    return square_bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_normalization(img):\n",
    "    \"\"\"Preprocessing step before feeding the network. \"\"\"\n",
    "    img = img.transpose((2, 0, 1))\n",
    "    img = np.expand_dims(img, 0)\n",
    "    # *0.0078125 i.e. 除以128\n",
    "    img = (img - 127.5) * 0.0078125\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IoU(box, boxes):\n",
    "    \"\"\"\n",
    "    Compute IoU between detect box and gt boxes\n",
    "    \"\"\"\n",
    "    # box = (x1, y1, x2, y2)\n",
    "    box_area = (box[2] - box[0] + 1) * (box[3] - box[1] + 1)\n",
    "    area = (boxes[:, 2] - boxes[:, 0] + 1) * (boxes[:, 3] - boxes[:, 1] + 1)\n",
    "\n",
    "    # abtain the offset of the interception of union between crop_box and gt_box\n",
    "    xx1 = np.maximum(box[0], boxes[:, 0])\n",
    "    yy1 = np.maximum(box[1], boxes[:, 1])\n",
    "    xx2 = np.minimum(box[2], boxes[:, 2])\n",
    "    yy2 = np.minimum(box[3], boxes[:, 3])\n",
    "\n",
    "    # compute the width and height of the bounding box\n",
    "    w = np.maximum(0, xx2 - xx1 + 1)\n",
    "    h = np.maximum(0, yy2 - yy1 + 1)\n",
    "\n",
    "    inter = w * h\n",
    "    ovr = inter / (box_area + area - inter)\n",
    "    return ovr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.ToTensor()\n",
    "from torch.autograd.variable import Variable\n",
    "def convert_image_to_tensor(image):\n",
    "    \"\"\"convert an image to pytorch tensor\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        image: numpy array , h * w * c\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        image_tensor: pytorch.FloatTensor, c * h * w\n",
    "        \"\"\"\n",
    "    # image = image.astype(np.float32)\n",
    "    return transform(image)\n",
    "    # return transform(image)\n",
    "def convert_chwTensor_to_hwcNumpy(tensor):\n",
    "    \"\"\"convert a group images pytorch tensor(count * c * h * w) to numpy array images(count * h * w * c)\n",
    "            Parameters:\n",
    "            ----------\n",
    "            tensor: numpy array , count * c * h * w\n",
    "\n",
    "            Returns:\n",
    "            -------\n",
    "            numpy array images: count * h * w * c\n",
    "            \"\"\"\n",
    "\n",
    "    if isinstance(tensor, Variable):\n",
    "        return np.transpose(tensor.data.numpy(), (0,2,3,1))\n",
    "    elif isinstance(tensor, torch.FloatTensor):\n",
    "        return np.transpose(tensor.numpy(), (0,2,3,1))\n",
    "    else:\n",
    "        raise Exception(\"covert b*c*h*w tensor to b*h*w*c numpy error.This tensor must have 4 dimension.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vis_face(im_array, dets, landmarks, save_name):\n",
    "    \"\"\"Visualize detection results before and after calibration\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    im_array: numpy.ndarray, shape(1, c, h, w)\n",
    "        test image in rgb\n",
    "    dets1: numpy.ndarray([[x1 y1 x2 y2 score]])\n",
    "        detection results before calibration\n",
    "    dets2: numpy.ndarray([[x1 y1 x2 y2 score]])\n",
    "        detection results after calibration\n",
    "    thresh: float\n",
    "        boxes with scores > thresh will be drawn in red otherwise yellow\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    \"\"\"\n",
    "    import pylab\n",
    "    landmarks=None\n",
    "    figure = pylab.figure()\n",
    "    # plt.subplot(121)\n",
    "    pylab.imshow(im_array)\n",
    "\n",
    "    for i in range(dets.shape[0]):\n",
    "        bbox = dets[i, :4]\n",
    "\n",
    "        rect = pylab.Rectangle((bbox[0], bbox[1]),\n",
    "                             bbox[2] - bbox[0],\n",
    "                             bbox[3] - bbox[1], fill=False,\n",
    "                             edgecolor='yellow', linewidth=0.9)\n",
    "        pylab.gca().add_patch(rect)\n",
    "\n",
    "    if landmarks is not None:\n",
    "        for i in range(landmarks.shape[0]):\n",
    "            landmarks_one = landmarks[i, :]\n",
    "            landmarks_one = landmarks_one.reshape((5, 2))\n",
    "            for j in range(5):\n",
    "                cir1 = Circle(xy=(landmarks_one[j, 0], landmarks_one[j, 1]), radius=2, alpha=0.4, color=\"red\")\n",
    "                pylab.gca().add_patch(cir1)\n",
    "    pylab.axis(\"off\")\n",
    "    pylab.savefig(save_name)\n",
    "    pylab.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MTCNN-Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MtcnnDetector(object):\n",
    "    \"\"\"\n",
    "        P,R,O net face detection and landmarks align\n",
    "    \"\"\"\n",
    "    def  __init__(self,\n",
    "                 pnet = None,\n",
    "                 rnet = None,\n",
    "                 onet = None,\n",
    "                 min_face_size=12,\n",
    "                 stride=2,\n",
    "                 threshold=[0.6, 0.7, 0.7],\n",
    "                 scale_factor=0.709,\n",
    "                 ):\n",
    "\n",
    "        self.pnet_detector = pnet\n",
    "        self.rnet_detector = rnet\n",
    "        self.onet_detector = onet\n",
    "        self.min_face_size = min_face_size\n",
    "        self.stride=stride\n",
    "        self.thresh = threshold\n",
    "        self.scale_factor = scale_factor\n",
    "\n",
    "    def generate_bounding_box(self, map, reg, scale, threshold):\n",
    "        \"\"\"\n",
    "            generate bbox from feature map\n",
    "        Parameters:\n",
    "        ----------\n",
    "            map: numpy array , n x m x 1\n",
    "                detect score for each position\n",
    "            reg: numpy array , n x m x 4\n",
    "                bbox\n",
    "            scale: float number\n",
    "                scale of this detection\n",
    "            threshold: float number\n",
    "                detect threshold\n",
    "        Returns:\n",
    "        -------\n",
    "            bbox array\n",
    "        \"\"\"\n",
    "        stride = 2\n",
    "        cellsize = 12 # receptive field\n",
    "\n",
    "        t_index = np.where(map[:,:,0] > threshold)\n",
    "        # find nothing\n",
    "        if t_index[0].size == 0:\n",
    "            return np.array([])\n",
    "        # choose bounding box whose socre are larger than threshold\n",
    "        dx1, dy1, dx2, dy2 = [reg[0, t_index[0], t_index[1], i] for i in range(4)]\n",
    "        #print(dx1.shape)\n",
    "        #exit()\n",
    "        # time.sleep(5)\n",
    "        reg = np.array([dx1, dy1, dx2, dy2])\n",
    "        score = map[t_index[0], t_index[1], 0]\n",
    "        # hence t_index[1] means column, t_index[1] is the value of x\n",
    "        # hence t_index[0] means row, t_index[0] is the value of y\n",
    "        boundingbox = np.vstack([np.round((stride * t_index[1]) / scale),            # x1 of prediction box in original image\n",
    "                                 np.round((stride * t_index[0]) / scale),            # y1 of prediction box in original image\n",
    "                                 np.round((stride * t_index[1] + cellsize) / scale), # x2 of prediction box in original image\n",
    "                                 np.round((stride * t_index[0] + cellsize) / scale), # y2 of prediction box in original image\n",
    "                                # reconstruct the box in original image\n",
    "                                 score,\n",
    "                                 reg,\n",
    "                                 # landmarks\n",
    "                                 ])\n",
    "\n",
    "        return boundingbox.T\n",
    "\n",
    "\n",
    "    def resize_image(self, img, scale):\n",
    "        \"\"\"\n",
    "            resize image and transform dimention to [batchsize, channel, height, width]\n",
    "        Parameters:\n",
    "        ----------\n",
    "            img: numpy array , height x width x channel\n",
    "                input image, channels in BGR order here\n",
    "            scale: float number\n",
    "                scale factor of resize operation\n",
    "        Returns:\n",
    "        -------\n",
    "            transformed image tensor , 1 x channel x height x width\n",
    "        \"\"\"\n",
    "        height, width, channels = img.shape\n",
    "        new_height = int(height * scale)     # resized new height\n",
    "        new_width = int(width * scale)       # resized new width\n",
    "        new_dim = (new_width, new_height)\n",
    "        img_resized = cv2.resize(img, new_dim, interpolation=cv2.INTER_LINEAR)      # resized image\n",
    "        return img_resized\n",
    "\n",
    "\n",
    "    def pad(self, bboxes, w, h):\n",
    "        \"\"\"\n",
    "            pad the the boxes\n",
    "        Parameters:\n",
    "        ----------\n",
    "            bboxes: numpy array, n x 5\n",
    "                input bboxes\n",
    "            w: float number\n",
    "                width of the input image\n",
    "            h: float number\n",
    "                height of the input image\n",
    "        Returns :\n",
    "        ------\n",
    "            dy, dx : numpy array, n x 1\n",
    "                start point of the bbox in target image\n",
    "            edy, edx : numpy array, n x 1\n",
    "                end point of the bbox in target image\n",
    "            y, x : numpy array, n x 1\n",
    "                start point of the bbox in original image\n",
    "            ex, ex : numpy array, n x 1\n",
    "                end point of the bbox in original image\n",
    "            tmph, tmpw: numpy array, n x 1\n",
    "                height and width of the bbox\n",
    "        \"\"\"\n",
    "        # width and height\n",
    "        tmpw = (bboxes[:, 2] - bboxes[:, 0] + 1).astype(np.int32)\n",
    "        tmph = (bboxes[:, 3] - bboxes[:, 1] + 1).astype(np.int32)\n",
    "        numbox = bboxes.shape[0]\n",
    "\n",
    "        dx = np.zeros((numbox, ))\n",
    "        dy = np.zeros((numbox, ))\n",
    "        edx, edy  = tmpw.copy()-1, tmph.copy()-1\n",
    "        # x, y: start point of the bbox in original image\n",
    "        # ex, ey: end point of the bbox in original image\n",
    "        x, y, ex, ey = bboxes[:, 0], bboxes[:, 1], bboxes[:, 2], bboxes[:, 3]\n",
    "\n",
    "        tmp_index = np.where(ex > w-1)\n",
    "        edx[tmp_index] = tmpw[tmp_index] + w - 2 - ex[tmp_index]\n",
    "        ex[tmp_index] = w - 1\n",
    "\n",
    "        tmp_index = np.where(ey > h-1)\n",
    "        edy[tmp_index] = tmph[tmp_index] + h - 2 - ey[tmp_index]\n",
    "        ey[tmp_index] = h - 1\n",
    "\n",
    "        tmp_index = np.where(x < 0)\n",
    "        dx[tmp_index] = 0 - x[tmp_index]\n",
    "        x[tmp_index] = 0\n",
    "\n",
    "        tmp_index = np.where(y < 0)\n",
    "        dy[tmp_index] = 0 - y[tmp_index]\n",
    "        y[tmp_index] = 0\n",
    "\n",
    "        return_list = [dy, edy, dx, edx, y, ey, x, ex, tmpw, tmph]\n",
    "        return_list = [item.astype(np.int32) for item in return_list]\n",
    "\n",
    "        return return_list\n",
    "\n",
    "\n",
    "    def detect_pnet(self, im):\n",
    "        \"\"\"Get face candidates through pnet\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        im: numpy array\n",
    "            input image array\n",
    "            one batch\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        boxes: numpy array\n",
    "            detected boxes before calibration\n",
    "        boxes_align: numpy array\n",
    "            boxes after calibration\n",
    "        \"\"\"\n",
    "        # original wider face data\n",
    "        im = cv2.cvtColor(np.asarray(im),cv2.COLOR_RGB2BGR)\n",
    "        h, w, c = im.shape\n",
    "        net_size = 12\n",
    "        current_scale = float(net_size) / self.min_face_size    # find initial scale\n",
    "        #print('imgshape:{0}, current_scale:{1}'.format(im.shape, current_scale))\n",
    "        im_resized = self.resize_image(im, current_scale) # scale = 1.0\n",
    "        current_height, current_width, _ = im_resized.shape\n",
    "        # fcn\n",
    "        all_boxes = list()\n",
    "        while min(current_height, current_width) > net_size:\n",
    "            #print('current:',current_height, current_width)\n",
    "            feed_imgs = []\n",
    "            image_tensor =convert_image_to_tensor(im_resized)\n",
    "            feed_imgs.append(image_tensor)\n",
    "            feed_imgs = torch.stack(feed_imgs).to(device)\n",
    "            # self.pnet_detector is a trained pnet torch model\n",
    "            # receptive field is 12×12\n",
    "            # 12×12 --> score\n",
    "            # 12×12 --> bounding box\n",
    "            cls_map, reg = self.pnet_detector(feed_imgs)\n",
    "\n",
    "            cls_map_np = convert_chwTensor_to_hwcNumpy(cls_map.cpu())\n",
    "            reg_np = convert_chwTensor_to_hwcNumpy(reg.cpu())\n",
    "            \n",
    "            # boxes = [x1, y1, x2, y2, score, reg]\n",
    "            boxes = self.generate_bounding_box(cls_map_np[ 0, :, :], reg_np, current_scale, self.thresh[0])\n",
    "           \n",
    "            # generate pyramid images\n",
    "            current_scale *= self.scale_factor # self.scale_factor = 0.709\n",
    "            im_resized = self.resize_image(im, current_scale)\n",
    "            current_height, current_width, _ = im_resized.shape\n",
    "\n",
    "            if boxes.size == 0:\n",
    "                continue\n",
    "\n",
    "            # non-maximum suppresion\n",
    "            keep = nms(boxes[:, :5], 0.5, 'Union')\n",
    "            boxes = boxes[keep]\n",
    "            all_boxes.append(boxes)\n",
    "\n",
    "        if len(all_boxes) == 0:\n",
    "            return None, None\n",
    "        all_boxes = np.vstack(all_boxes)\n",
    "        \n",
    "        # merge the detection from first stage\n",
    "        keep = nms(all_boxes[:, 0:5], 0.7, 'Union')\n",
    "        all_boxes = all_boxes[keep]\n",
    "        # boxes = all_boxes[:, :5]\n",
    "\n",
    "        # x2 - x1\n",
    "        # y2 - y1\n",
    "        bw = all_boxes[:, 2] - all_boxes[:, 0] + 1\n",
    "        bh = all_boxes[:, 3] - all_boxes[:, 1] + 1\n",
    "\n",
    "        # landmark_keep = all_boxes[:, 9:].reshape((5,2))\n",
    "        boxes = np.vstack([all_boxes[:,0],\n",
    "                   all_boxes[:,1],\n",
    "                   all_boxes[:,2],\n",
    "                   all_boxes[:,3],\n",
    "                   all_boxes[:,4]\n",
    "                  ])\n",
    "\n",
    "        boxes = boxes.T\n",
    "        # boxes = boxes = [x1, y1, x2, y2, score, reg] reg= [px1, py1, px2, py2] (in prediction)\n",
    "        align_topx = all_boxes[:, 0] + all_boxes[:, 5] * bw\n",
    "        align_topy = all_boxes[:, 1] + all_boxes[:, 6] * bh\n",
    "        align_bottomx = all_boxes[:, 2] + all_boxes[:, 7] * bw\n",
    "        align_bottomy = all_boxes[:, 3] + all_boxes[:, 8] * bh\n",
    "\n",
    "        # refine the boxes\n",
    "        boxes_align = np.vstack([ align_topx,\n",
    "                              align_topy,\n",
    "                              align_bottomx,\n",
    "                              align_bottomy,\n",
    "                              all_boxes[:, 4]\n",
    "                              ])\n",
    "        boxes_align = boxes_align.T\n",
    "\n",
    "        #remove invalid box\n",
    "        valindex = [True for _ in range(boxes_align.shape[0])]   \n",
    "        for i in range(boxes_align.shape[0]):\n",
    "            if boxes_align[i][2]-boxes_align[i][0]<=3 or boxes_align[i][3]-boxes_align[i][1]<=3:\n",
    "                valindex[i]=False\n",
    "                print('pnet has one smaller than 3')\n",
    "            else:\n",
    "                if boxes_align[i][2]<1 or boxes_align[i][0]>w-2 or boxes_align[i][3]<1 or boxes_align[i][1]>h-2:\n",
    "                    valindex[i]=False\n",
    "                    print('pnet has one out')\n",
    "        boxes_align=boxes_align[valindex,:]\n",
    "        boxes = boxes[valindex,:]\n",
    "        return boxes, boxes_align\n",
    "\n",
    "    def detect_rnet(self, im, dets):\n",
    "        \"\"\"Get face candidates using rnet\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        im: numpy array\n",
    "            input image array\n",
    "        dets: numpy array\n",
    "            detection results of pnet\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        boxes: numpy array\n",
    "            detected boxes before calibration\n",
    "        boxes_align: numpy array\n",
    "            boxes after calibration\n",
    "        \"\"\"\n",
    "        # im: an input image\n",
    "        im = cv2.cvtColor(np.asarray(im),cv2.COLOR_RGB2BGR)\n",
    "        h, w, c = im.shape\n",
    "\n",
    "        if dets is None:\n",
    "            return None,None\n",
    "        if dets.shape[0]==0:\n",
    "            return None, None\n",
    "        detss = dets\n",
    "        # return square boxes\n",
    "        dets = convert_to_square(dets)\n",
    "        detsss = dets\n",
    "        # rounds\n",
    "        dets[:, 0:4] = np.round(dets[:, 0:4])\n",
    "        [dy, edy, dx, edx, y, ey, x, ex, tmpw, tmph] = self.pad(dets, w, h)\n",
    "        num_boxes = dets.shape[0]\n",
    "        # cropped_ims_tensors = np.zeros((num_boxes, 3, 24, 24), dtype=np.float32)\n",
    "        cropped_ims_tensors = []\n",
    "        for i in range(num_boxes):\n",
    "            try:\n",
    "                tmp = np.zeros((tmph[i], tmpw[i], 3), dtype=np.uint8)\n",
    "                tmp[dy[i]:edy[i]+1, dx[i]:edx[i]+1, :] = im[y[i]:ey[i]+1, x[i]:ex[i]+1, :]\n",
    "            except:    \n",
    "                print(dy[i],edy[i],dx[i],edx[i],y[i],ey[i],x[i],ex[i],tmpw[i],tmph[i])\n",
    "                print(dets[i])\n",
    "                print(detss[i])\n",
    "                print(detsss[i])\n",
    "                print(h,w)\n",
    "                exit()\n",
    "            crop_im = cv2.resize(tmp, (24, 24))\n",
    "            crop_im_tensor = convert_image_to_tensor(crop_im)\n",
    "            # cropped_ims_tensors[i, :, :, :] = crop_im_tensor\n",
    "            cropped_ims_tensors.append(crop_im_tensor)\n",
    "        feed_imgs = torch.stack(cropped_ims_tensors).to(device)\n",
    "       \n",
    "\n",
    "        cls_map, reg = self.rnet_detector(feed_imgs)\n",
    "        cls_map = cls_map.cpu().data.numpy()\n",
    "        reg = reg.cpu().data.numpy()\n",
    "        # landmark = landmark.cpu().data.numpy()\n",
    "        \n",
    "        keep_inds = np.where(cls_map > self.thresh[1])[0]\n",
    "        if len(keep_inds) > 0:\n",
    "            boxes = dets[keep_inds]\n",
    "            cls = cls_map[keep_inds]\n",
    "            reg = reg[keep_inds]\n",
    "            # landmark = landmark[keep_inds]\n",
    "        else:\n",
    "            return None, None\n",
    "        keep = nms(boxes, 0.7)\n",
    "\n",
    "        if len(keep) == 0:\n",
    "            return None, None\n",
    "\n",
    "        keep_cls = cls[keep]\n",
    "        keep_boxes = boxes[keep]\n",
    "        keep_reg = reg[keep]\n",
    "        # keep_landmark = landmark[keep]\n",
    "\n",
    "\n",
    "        bw = keep_boxes[:, 2] - keep_boxes[:, 0] + 1\n",
    "        bh = keep_boxes[:, 3] - keep_boxes[:, 1] + 1\n",
    "\n",
    "\n",
    "        boxes = np.vstack([ keep_boxes[:,0],\n",
    "                              keep_boxes[:,1],\n",
    "                              keep_boxes[:,2],\n",
    "                              keep_boxes[:,3],\n",
    "                              keep_cls[:,0]\n",
    "                            ])\n",
    "\n",
    "        align_topx = keep_boxes[:,0] + keep_reg[:,0] * bw\n",
    "        align_topy = keep_boxes[:,1] + keep_reg[:,1] * bh\n",
    "        align_bottomx = keep_boxes[:,2] + keep_reg[:,2] * bw\n",
    "        align_bottomy = keep_boxes[:,3] + keep_reg[:,3] * bh\n",
    "\n",
    "        boxes_align = np.vstack([align_topx,\n",
    "                               align_topy,\n",
    "                               align_bottomx,\n",
    "                               align_bottomy,\n",
    "                               keep_cls[:, 0]\n",
    "                             ])\n",
    "\n",
    "        boxes = boxes.T\n",
    "        boxes_align = boxes_align.T\n",
    "\n",
    "        #remove invalid box\n",
    "        valindex = [True for _ in range(boxes_align.shape[0])]   \n",
    "        for i in range(boxes_align.shape[0]):\n",
    "            if boxes_align[i][2]-boxes_align[i][0]<=3 or boxes_align[i][3]-boxes_align[i][1]<=3:\n",
    "                valindex[i]=False\n",
    "                print('rnet has one smaller than 3')\n",
    "            else:\n",
    "                if boxes_align[i][2]<1 or boxes_align[i][0]>w-2 or boxes_align[i][3]<1 or boxes_align[i][1]>h-2:\n",
    "                    valindex[i]=False\n",
    "                    print('rnet has one out')\n",
    "        boxes_align=boxes_align[valindex,:]\n",
    "        boxes = boxes[valindex,:]\n",
    "        return boxes, boxes_align\n",
    "\n",
    "    def detect_onet(self, im, dets):\n",
    "        \"\"\"Get face candidates using onet\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        im: numpy array\n",
    "            input image array\n",
    "        dets: numpy array\n",
    "            detection results of rnet\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        boxes_align: numpy array\n",
    "            boxes after calibration\n",
    "        landmarks_align: numpy array\n",
    "            landmarks after calibration\n",
    "\n",
    "        \"\"\"\n",
    "        im = cv2.cvtColor(np.asarray(im),cv2.COLOR_RGB2BGR) \n",
    "        h, w, c = im.shape\n",
    "\n",
    "        if dets is None:\n",
    "            return None, None\n",
    "        if dets.shape[0]==0:\n",
    "            return None, None\n",
    "\n",
    "        detss = dets\n",
    "        dets = convert_to_square(dets)\n",
    "        \n",
    "        \n",
    "        dets[:, 0:4] = np.round(dets[:, 0:4])\n",
    "\n",
    "        [dy, edy, dx, edx, y, ey, x, ex, tmpw, tmph] = self.pad(dets, w, h)\n",
    "        num_boxes = dets.shape[0]\n",
    "\n",
    "\n",
    "        # cropped_ims_tensors = np.zeros((num_boxes, 3, 24, 24), dtype=np.float32)\n",
    "        cropped_ims_tensors = []\n",
    "        for i in range(num_boxes):\n",
    "            try:\n",
    "                tmp = np.zeros((tmph[i], tmpw[i], 3), dtype=np.uint8)\n",
    "                # crop input image\n",
    "                tmp[dy[i]:edy[i] + 1, dx[i]:edx[i] + 1, :] = im[y[i]:ey[i] + 1, x[i]:ex[i] + 1, :]\n",
    "            except:\n",
    "                print(dy[i],edy[i],dx[i],edx[i],y[i],ey[i],x[i],ex[i],tmpw[i],tmph[i])\n",
    "                print(dets[i])\n",
    "                print(detss[i])\n",
    "                print(h,w)\n",
    "            crop_im = cv2.resize(tmp, (48, 48))\n",
    "            crop_im_tensor = convert_image_to_tensor(crop_im)\n",
    "            # cropped_ims_tensors[i, :, :, :] = crop_im_tensor\n",
    "            cropped_ims_tensors.append(crop_im_tensor)\n",
    "        feed_imgs = torch.stack(cropped_ims_tensors)\n",
    "        feed_imgs = feed_imgs.to(device)\n",
    "\n",
    "        cls_map, reg, landmark = self.onet_detector(feed_imgs)\n",
    "\n",
    "        cls_map = cls_map.cpu().data.numpy()\n",
    "        reg = reg.cpu().data.numpy()\n",
    "        landmark = landmark.cpu().data.numpy()\n",
    "\n",
    "        keep_inds = np.where(cls_map > self.thresh[2])[0]\n",
    "\n",
    "        if len(keep_inds) > 0:\n",
    "            boxes = dets[keep_inds]\n",
    "            cls = cls_map[keep_inds]\n",
    "            reg = reg[keep_inds]\n",
    "            landmark = landmark[keep_inds]\n",
    "        else:\n",
    "            return None, None\n",
    "\n",
    "        keep = nms(boxes, 0.7, mode=\"Minimum\")\n",
    "\n",
    "        if len(keep) == 0:\n",
    "            return None, None\n",
    "\n",
    "        keep_cls = cls[keep]\n",
    "        keep_boxes = boxes[keep]\n",
    "        keep_reg = reg[keep]\n",
    "        keep_landmark = landmark[keep]\n",
    "\n",
    "        bw = keep_boxes[:, 2] - keep_boxes[:, 0] + 1\n",
    "        bh = keep_boxes[:, 3] - keep_boxes[:, 1] + 1\n",
    "\n",
    "\n",
    "        align_topx = keep_boxes[:, 0] + keep_reg[:, 0] * bw\n",
    "        align_topy = keep_boxes[:, 1] + keep_reg[:, 1] * bh\n",
    "        align_bottomx = keep_boxes[:, 2] + keep_reg[:, 2] * bw\n",
    "        align_bottomy = keep_boxes[:, 3] + keep_reg[:, 3] * bh\n",
    "\n",
    "        align_landmark_topx = keep_boxes[:, 0]\n",
    "        align_landmark_topy = keep_boxes[:, 1]\n",
    "\n",
    "        boxes_align = np.vstack([align_topx,\n",
    "                                 align_topy,\n",
    "                                 align_bottomx,\n",
    "                                 align_bottomy,\n",
    "                                 keep_cls[:, 0]\n",
    "                                 ])\n",
    "\n",
    "        boxes_align = boxes_align.T\n",
    "\n",
    "        landmark =  np.vstack([\n",
    "                                 align_landmark_topx + keep_landmark[:, 0] * bw,\n",
    "                                 align_landmark_topy + keep_landmark[:, 1] * bh,\n",
    "                                 align_landmark_topx + keep_landmark[:, 2] * bw,\n",
    "                                 align_landmark_topy + keep_landmark[:, 3] * bh,\n",
    "                                 align_landmark_topx + keep_landmark[:, 4] * bw,\n",
    "                                 align_landmark_topy + keep_landmark[:, 5] * bh,\n",
    "                                 align_landmark_topx + keep_landmark[:, 6] * bw,\n",
    "                                 align_landmark_topy + keep_landmark[:, 7] * bh,\n",
    "                                 align_landmark_topx + keep_landmark[:, 8] * bw,\n",
    "                                 align_landmark_topy + keep_landmark[:, 9] * bh,\n",
    "                                 ])\n",
    "\n",
    "        landmark_align = landmark.T\n",
    "\n",
    "        return boxes_align, landmark_align\n",
    "\n",
    "\n",
    "    def detect_face(self,img):\n",
    "        \"\"\"Detect face over image\n",
    "        \"\"\"\n",
    "        boxes_align = np.array([])\n",
    "        landmark_align =np.array([])\n",
    "\n",
    "        t = time.time()\n",
    "\n",
    "        # pnet\n",
    "        if self.pnet_detector:\n",
    "            boxes, boxes_align = self.detect_pnet(img)\n",
    "            if boxes_align is None:\n",
    "                return np.array([]), np.array([])\n",
    "\n",
    "            t1 = time.time() - t\n",
    "            t = time.time()\n",
    "\n",
    "        # rnet\n",
    "        if self.rnet_detector:\n",
    "            boxes, boxes_align = self.detect_rnet(img, boxes_align)\n",
    "            if boxes_align is None:\n",
    "                return np.array([]), np.array([])\n",
    "\n",
    "            t2 = time.time() - t\n",
    "            t = time.time()\n",
    "\n",
    "        # onet\n",
    "        if self.onet_detector:\n",
    "            boxes_align, landmark_align = self.detect_onet(img, boxes_align)\n",
    "            if boxes_align is None:\n",
    "                return np.array([]), np.array([])\n",
    "\n",
    "            t3 = time.time() - t\n",
    "            t = time.time()\n",
    "            print(\"time cost \" + '{:.3f}'.format(t1+t2+t3) + '  pnet {:.3f}  rnet {:.3f}  onet {:.3f}'.format(t1, t2, t3))\n",
    "\n",
    "        return boxes_align, landmark_align\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 图片的预处理："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.对要输入P-Net图片的预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pnet_data_txt_parser(txt_path, img_dir):\n",
    "    \"\"\"\n",
    "    :param txt_path: the path of wider_face_train_bbx_gt.txt\n",
    "    :param img_dir: tha dir of WIDER/WIDER_train\n",
    "    :return: img_faces type is list, shape is [img_num*[absolute_img_path,[faces_num*4(which is x1,y1,w,h)]]]\n",
    "    \"\"\"\n",
    "    if osp.exists(txt_path):\n",
    "        # *** img_faces shape :[img_path,[faces_num, 4]]\n",
    "        img_faces = []\n",
    "        with open(txt_path, 'r') as f:\n",
    "            l = []\n",
    "            lines = list(map(lambda line: line.strip().split('\\n'), f))\n",
    "            # lines[[str],[str],[]...]\n",
    "            lines = [i[0] for i in lines]\n",
    "            # lines [str,str...]\n",
    "            line_counter = 0\n",
    "            img_count=0\n",
    "            while line_counter < len(lines):\n",
    "                img_path = lines[line_counter]\n",
    "                img_count+=1\n",
    "                faces_pos = []\n",
    "                faces_num = int(lines[line_counter + 1])\n",
    "                if faces_num==0:\n",
    "                    print(\"Find a picture with no face in it.\")\n",
    "                    line_counter +=1        \n",
    "                for i in range(faces_num):\n",
    "                    face_pos = lines[line_counter + 1 + i + 1].split()\n",
    "                    # [x1, y1, w, h]\n",
    "                    face_pos = face_pos[:4]\n",
    "                    face_pos = [int(i) for i in face_pos]\n",
    "                    faces_pos.append(face_pos)\n",
    "                real_img_path = osp.join(img_dir, img_path)\n",
    "                if osp.exists(real_img_path) and faces_num!=0:\n",
    "                    try:\n",
    "                        Image.open(real_img_path).verify()\n",
    "                        img_faces.append([real_img_path, faces_pos])\n",
    "                    except:\n",
    "                        print(\"*** warning:cannot open\",real_img_path)\n",
    "                else:\n",
    "                    print(\"*** warning:image path invalid\")\n",
    "                line_counter += (2 + faces_num)\n",
    "                \n",
    "##########################################用来控制选取多少图片来训练\n",
    "                if img_count>=3000:\n",
    "                    break\n",
    "        return img_faces\n",
    "    else:\n",
    "        print('*** warning:WILDER_FACE txt file not exist!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.对Landmark图片的预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def landmark_dataset_txt_parser(txt_path, img_dir):\n",
    "    \"\"\"\n",
    "    :param txt_path:\n",
    "    :param img_dir:\n",
    "    :return: [absolute_img_path,[x1,x2,y1,y2],(x,y)of[left_eye,right_eye,nose,mouse_left, mouse_right]]\n",
    "    \"\"\"\n",
    "    if txt_path is None or img_dir is None:\n",
    "        return []\n",
    "    if osp.exists(txt_path):\n",
    "        # *** img_faces shape :[img_path,[faces_num, 4]]\n",
    "        img_faces = []\n",
    "        with open(txt_path, 'r') as f:\n",
    "            l = []\n",
    "            lines = list(map(lambda line: line.strip().split('\\n'), f))\n",
    "            # lines[[str],[str],[]...]\n",
    "            lines = [i[0].split(' ') for i in lines]\n",
    "            # lines [[path_str,pos_str]...]\n",
    "            for line in lines:\n",
    "                # 将路径中的'\\'替换为'/'\n",
    "                img_path = line[0].replace('\\\\', '/')\n",
    "                faces_pos = [int(i) for i in line[1:5]]\n",
    "                # 标注为 左右眼，嘴，左右嘴角\n",
    "                landmark = [float(i) for i in line[5:]]\n",
    "                real_img_path = osp.join(img_dir, img_path)\n",
    "                # if DEBUG: print(real_img_path)\n",
    "                # if DEBUG: print(osp.exists(real_img_path), Image.open(real_img_path).verify())\n",
    "                if osp.exists(real_img_path):\n",
    "                    try:\n",
    "                        Image.open(real_img_path).verify()\n",
    "                        img_faces.append([real_img_path, faces_pos, landmark])\n",
    "                    except:\n",
    "                        print('Invalid image',real_img_path)\n",
    "                else:\n",
    "                    print(\"*** warning:image path invalid\")\n",
    "\n",
    "        # for i in img_faces: print(i)\n",
    "        return img_faces\n",
    "    else:\n",
    "        print('*** warning:WILDER_FACE txt file not exist!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 对demo的图片进行预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_widerdata(data, num_images, Random=True):\n",
    "    \"\"\"\n",
    "    data: train, valid\n",
    "    \"\"\"\n",
    "    if data==\"train\":\n",
    "        img_faces = create_pnet_data_txt_parser(args.class_traindata_txt_path, args.class_traindata_dir)\n",
    "        print(\"get train images\")\n",
    "    elif data==\"valid\":\n",
    "        img_faces = create_pnet_data_txt_parser(args.class_validdata_txt_path, args.class_validdata_dir)\n",
    "        print(\"get valid images\")\n",
    "        \n",
    "    total=len(img_faces)\n",
    "    result=[]\n",
    "    indexes=[]\n",
    "    while len(result)<num_images:\n",
    "        if Random: \n",
    "            index=random.randint(0,total-1)\n",
    "            if index not in indexes:\n",
    "                result.append(img_faces[index])\n",
    "        else:\n",
    "            for i in range(num_images):\n",
    "                result.append(img_faces[i])\n",
    "            \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# print 图片的functions："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 比较ground truth 和 pred_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imageshow(args,data,num_images,Random,net_name,save_name):\n",
    "    \"\"\"\n",
    "    :param data:可以选 \"train\" \"valid\"\n",
    "    :param num——images: print的image的数量\n",
    "    :param Random: 是否Random抽取图片\n",
    "    :param net_name: 想可视化P/R/Onet中的哪个的训练效果\n",
    "    :param save_name: 图片保存的名字\n",
    "    \"\"\"\n",
    "    pnet= load_net(args, 'pnet')\n",
    "    landmarks=None\n",
    "    data_for_demo = get_widerdata(data, num_images,Random)\n",
    "    num_demo = len(data_for_demo)\n",
    "    fig, ax = plt.subplots(num_demo,2,dpi = 600, figsize=(2,num_demo))\n",
    "    for i in range(num_demo):\n",
    "        img_pth, gts = data_for_demo[i]\n",
    "        img = Image.open(img_pth)\n",
    "        mtcnn_detector = MtcnnDetector(pnet=pnet,min_face_size=12)\n",
    "        boxes, bounding_boxes = mtcnn_detector.detect_pnet(im=img)\n",
    "        if net_name==\"rnet\":\n",
    "            rnet= load_net(args, 'rnet')\n",
    "            mtcnn_detector = MtcnnDetector(pnet=pnet,rnet=rnet,min_face_size=12)\n",
    "            boxes,bounding_boxes =mtcnn_detector.detect_rnet(im=img, dets=bounding_boxes)\n",
    "        elif net_name==\"onet\":\n",
    "            rnet= load_net(args, 'rnet')\n",
    "            onet= load_net(args, 'onet')\n",
    "            mtcnn_detector = MtcnnDetector(pnet=pnet,rnet=rnet,onet=onet,min_face_size=12)\n",
    "            boxes,r_bounding_boxes =mtcnn_detector.detect_rnet(im=img, dets=bounding_boxes)\n",
    "            bounding_boxes,landmarks =mtcnn_detector.detect_onet(im=img, dets=r_bounding_boxes)                   \n",
    "        ax[i,0].imshow(img)                    \n",
    "        for b in gts:\n",
    "            ax[i,0].add_patch(plt.Rectangle((b[0], b[1]), b[2], b[3], fill=False, edgecolor='cyan',linewidth=0.4))\n",
    "        ax[i,1].imshow(img)\n",
    "        try:\n",
    "            num_boxes=len(bounding_boxes)\n",
    "            ####这里可以改变打出来的框的数量哦！\n",
    "            for b in bounding_boxes:\n",
    "                ax[i,1].add_patch(plt.Rectangle((b[0], b[1]), b[2]-b[0], b[3]-b[1], fill=False, edgecolor='cyan',linewidth=0.4)) \n",
    "            if landmarks is not None:\n",
    "                for m in range(landmarks.shape[0]):\n",
    "                    landmarks_one = landmarks[m, :]\n",
    "                    landmarks_one = landmarks_one.reshape((5, 2))\n",
    "                    for j in range(5):\n",
    "                        cir1 = Circle(xy=(landmarks_one[j, 0], landmarks_one[j, 1]), radius=2, alpha=0.4, color=\"red\")\n",
    "                        ax[i,1].add_patch(cir1)    \n",
    "        except:\n",
    "            num_boxes=0\n",
    "        ax[i,1].text(0, 0, '# boxes: '+str(num_boxes),fontsize=3)    \n",
    "    for axes in ax.ravel():\n",
    "        axes.axis('off')\n",
    "    ax[0][0].set_title(\"Ground truth\",fontsize=5)\n",
    "    if net_name==\"pnet\":\n",
    "        ax[0][1].set_title(\"P-Net performance\",fontsize=5)\n",
    "    elif net_name==\"rnet\":\n",
    "        ax[0][1].set_title(\"R-Net performance\",fontsize=5)\n",
    "    elif net_name==\"onet\":\n",
    "        ax[0][1].set_title(\"O-Net performance\",fontsize=5)\n",
    "    plt.tight_layout() #让图片紧密 \n",
    "    fig.show()\n",
    "    fig.savefig(save_name+'_demo.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. print Loss图像"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_show(net_name,train_loss,valid_loss):\n",
    "    # visualize the loss as the network trained\n",
    "    fig = plt.figure(figsize=(10,8))\n",
    "    plt.plot(range(1,len(train_loss)+1),train_loss, label='Training Loss')\n",
    "    plt.plot(range(1,len(valid_loss)+1),valid_loss,label='Validation Loss')\n",
    "    plt.xlabel('epochs',fontsize=30)\n",
    "    plt.ylabel('loss',fontsize=30)\n",
    "\n",
    "    #plt.ylim(0, 0.5) # consistent scale\n",
    "    plt.xlim(1, len(train_loss)+1) # consistent scale\n",
    "    plt.yticks(fontsize=30)\n",
    "    plt.xticks(fontsize=30)\n",
    "    plt.grid(True)\n",
    "    plt.legend(loc = 'best',fontsize=30)\n",
    "    plt.tight_layout()\n",
    "    fig.savefig('{}_loss.png'.format(net_name), bbox_inches='tight',dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 设定Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InplaceDataset(data.Dataset):\n",
    "    def __init__(self, img_face_landmark, img_faces, cropsize, pnet=None, rnet=None, ratio=(2, 1, 1, 1)):\n",
    "        \"\"\"\n",
    "        :param train_data_list: [train_data_num,[img_path,labels,[offsets],[landmark]]\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.img_faces = img_face_landmark + img_faces\n",
    "        shuffle(self.img_faces)\n",
    "        self.crop_size = cropsize\n",
    "        self.pnet = pnet\n",
    "        self.rnet = rnet\n",
    "        ratio_sum = float(sum(ratio))\n",
    "        self.ratio = [i / ratio_sum for i in ratio]\n",
    "        self.cache = []\n",
    "        #print('===> data set size:{}'.format(self.__len__()))\n",
    "        # self.dict = {'p': 0.0, 'pf': 0.0, 'l': 1.0, 'n': 0.0}\n",
    "\n",
    "    def get_img_faces_ldmk(self, index):\n",
    "        def load_img(img_path):\n",
    "            try:\n",
    "                img = Image.open(img_path)\n",
    "                img = img.convert('RGB')\n",
    "            except Exception:\n",
    "                print('*** warning loading fail!')\n",
    "                return\n",
    "            return img\n",
    "\n",
    "        img_face = self.img_faces[index]\n",
    "        img_path = img_face[0]\n",
    "        #这个图片一共标注了多少个face\n",
    "        faces = np.array(img_face[1])\n",
    "        \n",
    "        \n",
    "      \n",
    "        if faces.ndim is 1:\n",
    "            # img_face_landmark\n",
    "            # [absolute_img_path,[x1,x2,y1,y2],(x,y)of[left_eye,right_eye,nose,mouse_left, mouse_right]]\n",
    "            try:\n",
    "                faces = np.expand_dims(faces, 0)\n",
    "                faces[:, :] = faces[:, (0, 2, 1, 3)]\n",
    "            except:\n",
    "                print('error:',img_path)\n",
    "            \n",
    "\n",
    "        else:\n",
    "            # [img_num * [absolute_img_path, [faces_num * 4(which is x1, y1, w, h)]]]\n",
    "            faces[:, 2] += faces[:, 0]\n",
    "            faces[:, 3] += faces[:, 1]\n",
    "        # print('faces:{}'.format(faces))\n",
    "        ldmk = None if len(img_face) < 3 else [int(i) for i in img_face[2]]\n",
    "\n",
    "        return load_img(img_path), faces, ldmk\n",
    "\n",
    "    def get_crop_img_label_offset_ldmk(self, img, faces, ldmk, index):\n",
    "        def get_crop_img(img_np, crop_box, crop_size):\n",
    "            crop_box = [int(i) for i in crop_box]\n",
    "            crop_img_np = img_np[crop_box[1]:crop_box[3], crop_box[0]:crop_box[2], :]\n",
    "            crop_img = Image.fromarray(crop_img_np, mode='RGB')\n",
    "            crop_img = crop_img.resize((crop_size, crop_size), resample=PIL.Image.BILINEAR)\n",
    "            return crop_img\n",
    "\n",
    "        def get_real_label(label):\n",
    "            return {'n': 'n', 'np': 'n', 'pf': 'pf' if ldmk is None else 'l',\n",
    "                    'p': 'p' if ldmk is None else 'l'}.get(label)\n",
    "\n",
    "        def cal_offset(face, box):\n",
    "            if box is None:\n",
    "                return []\n",
    "            offset = [\n",
    "                (face[0] - box[0]) / float(box[2] - box[0]),\n",
    "                (face[1] - box[1]) / float(box[3] - box[1]),\n",
    "                (face[2] - box[2]) / float(box[2] - box[0]),\n",
    "                (face[3] - box[3]) / float(box[3] - box[1]),\n",
    "            ]\n",
    "            return offset\n",
    "\n",
    "        def cal_landmark_offset(box, ldmk):\n",
    "            if ldmk is None or box is None:\n",
    "                return []\n",
    "            else:\n",
    "                minx, miny = box[0], box[1]\n",
    "                w, h = box[2] - box[0], box[3] - box[1]\n",
    "                ldmk_offset = [(ldmk[i] - [minx, miny][i % 2]) / float([w, h][i % 2]) for i in range(len(ldmk))]\n",
    "                # print('box:{},ldmk:{},ldmk_offset:{}'.format(box, ldmk, ldmk_offset))\n",
    "                return ldmk_offset\n",
    "\n",
    "        img_np = np.array(img)\n",
    "        width, height = img.size\n",
    "        \n",
    "        if self.pnet is None:\n",
    "            # negative, negative partial, partial face, positive\n",
    "            label = np.random.choice(['n', 'np', 'pf', 'p'], p=self.ratio)\n",
    "            # label = 'np'\n",
    "            # print('label:{}'.format(label))\n",
    "            iou_th = {'n': (0, 0.3), 'np': (0, 0.3), 'pf': (0.4, 0.65), 'p': (0.65, 1.0)}.get(label)\n",
    "            sigma = {'n': 1, 'np': 0.3, 'pf': 0.1, 'p': 0.02}.get(label)\n",
    "            face, face_max_size = None, None\n",
    "            for i in range(10):\n",
    "                face = faces[random.randint(0,len(faces)-1)]\n",
    "                face_max_size = max(face[2] - face[0], face[3] - face[1])\n",
    "                if face_max_size > self.crop_size:\n",
    "                    break\n",
    "            crop_img = None\n",
    "            crop_box = None\n",
    "            for i in range(10):\n",
    "                # if ct >= sample_num: break\n",
    "                max_size = min(width, height)\n",
    "                size = (uniform(-1.0, 1.0) * sigma + 1) * face_max_size\n",
    "                # 保证大于剪切的尺寸要大于一个值\n",
    "                size = min(max(self.crop_size, size), max_size)\n",
    "                # print('size:', size)\n",
    "                x1, y1 = face[0], face[1]\n",
    "                crop_x1, crop_y1 = (uniform(-1.0, 1.0) * sigma + 1) * x1, (uniform(-1.0, 1.0) * sigma + 1) * y1\n",
    "                crop_x1, crop_y1 = min(max(0, crop_x1), width - size), min(max(0, crop_y1), height - size)\n",
    "                crop_box = np.array([int(crop_x1), int(crop_y1), int(crop_x1 + size), int(crop_y1 + size)])\n",
    "                # print('crop_box:', crop_box)\n",
    "                # print('faces_two_points:', faces_two_points)\n",
    "                iou = IoU(crop_box, np.array([face]))\n",
    "                iou_max_idx = iou.argmax()\n",
    "                iou = iou.max()\n",
    "                # print('iou', iou)\n",
    "                # iou值不符则跳过\n",
    "                if iou < iou_th[0] or iou > iou_th[1]:\n",
    "                    continue\n",
    "                else:\n",
    "                    # print('img_np:{}'.format(img_np))\n",
    "                    crop_img = get_crop_img(img_np, crop_box, self.crop_size)\n",
    "                    # crop_img.show()\n",
    "                    break\n",
    "            return crop_img, get_real_label(label), cal_offset(face, crop_box), cal_landmark_offset(crop_box, ldmk)\n",
    "        else:\n",
    "            # negative, negative partial, partial face, positive\n",
    "            if len(self.cache) != 0:\n",
    "                self.img_faces.append(self.img_faces[index])\n",
    "                return self.cache.pop(0)\n",
    "            iou_th = {'n': (0, 0.3), 'pf': (0.4, 0.65), 'p': (0.65, 1.0)}\n",
    "            # sigma = {'n': 1, 'np': 0.3, 'pf': 0.1, 'p': 0.02} \n",
    "            mtcnn_detector = MtcnnDetector(pnet=self.pnet,min_face_size=12)\n",
    "            boxes, bounding_boxes = mtcnn_detector.detect_pnet(im=img)\n",
    "            if bounding_boxes is None:\n",
    "                return None, None, None, None\n",
    "            if self.rnet is not None:\n",
    "                mtcnn_detector = MtcnnDetector(pnet=self.pnet,rnet=self.rnet,min_face_size=12)\n",
    "                boxes,bounding_boxes_rnet =mtcnn_detector.detect_rnet(im=img, dets=bounding_boxes)\n",
    "                if bounding_boxes_rnet!= None:\n",
    "                    bounding_boxes = np.vstack((bounding_boxes, bounding_boxes_rnet))\n",
    "            crop_img = None\n",
    "            crop_box = None\n",
    "            closet_face = None\n",
    "            for id, box in enumerate(bounding_boxes, start=1):\n",
    "                box = [min(max(0, int(box[i])), width if i % 2 == 0 else height) for i in range(4)]\n",
    "                if box[2] - box[0] < self.crop_size: continue\n",
    "                iou = IoU(box, faces)\n",
    "                iou_max = iou.max()\n",
    "                iou_index = iou.argmax()\n",
    "                closet_face = faces[iou_index]\n",
    "                # print('iou_max:{}, iou_index:{}'.format(iou_max, iou_index))\n",
    "                # ioumax = max(iou, iou_max)\n",
    "                crop_img = get_crop_img(img_np=img_np, crop_box=box, crop_size=self.crop_size)\n",
    "                # img_box.show()\n",
    "                # [(0, 0.3), (0.4, 0.65), (0.65, 1.0)]\n",
    "                for temp_label in iou_th:\n",
    "                    if iou_max < iou_th[temp_label][0] or iou_max > iou_th[temp_label][1]:\n",
    "                        continue\n",
    "                    else:\n",
    "                        label = temp_label\n",
    "                        crop_box = box\n",
    "                        crop_img = get_crop_img(img_np, box, self.crop_size)\n",
    "                        self.cache.append((crop_img, get_real_label(label),\n",
    "                                           cal_offset(closet_face, crop_box), cal_landmark_offset(crop_box, ldmk)))\n",
    "\n",
    "            return (None, None, None, None) if len(self.cache) == 0 else self.cache.pop(0)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img, faces, ldmk = self.get_img_faces_ldmk(index)\n",
    "        crop_img, label, offset, ldmk = self.get_crop_img_label_offset_ldmk(img, faces, ldmk, index)\n",
    "        if crop_img is None: return self.__getitem__(random.randint(0, self.__len__()-1))\n",
    "        img_tensor = transforms.ToTensor()(crop_img)\n",
    "        landmark_flag = torch.FloatTensor([1.0 if label == 'l' else 0.0])\n",
    "        label = torch.FloatTensor([1.0 if label in ['p', 'pf', 'l'] else 0.0])\n",
    "        offset = torch.FloatTensor(offset if 4 == len(offset) else 4 * [0.0])\n",
    "        landmark = torch.FloatTensor(ldmk if 10 == len(ldmk) else 10 * [0.0])\n",
    "        return (img_tensor, label, offset, landmark_flag, landmark)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_faces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 设立dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 为了区分train data和valid data，此处引入了data这个参数来区分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inplace_data_loader(args, net_name,data=\"train\"):\n",
    "    if data==\"train\":\n",
    "        img_faces = create_pnet_data_txt_parser(args.class_traindata_txt_path, args.class_traindata_dir)\n",
    "        img_face_landmark = landmark_dataset_txt_parser(args.landmark_traindata_txt_path, args.landmark_traindata_dir)\n",
    "        if net_name == 'pnet':\n",
    "            IDS = InplaceDataset(img_face_landmark, img_faces, cropsize=12)\n",
    "        elif net_name == 'rnet':\n",
    "            IDS = InplaceDataset(img_face_landmark, img_faces, cropsize=24, pnet=load_net(args, 'pnet'))\n",
    "        elif net_name == 'onet':\n",
    "            IDS = InplaceDataset(img_face_landmark, img_faces, cropsize=48,\n",
    "                                 pnet=load_net(args, 'pnet'), rnet=load_net(args, 'rnet'))\n",
    "    elif data==\"valid\":\n",
    "        img_faces = create_pnet_data_txt_parser(args.class_validdata_txt_path, args.class_validdata_dir)\n",
    "        img_face_landmark = landmark_dataset_txt_parser(args.landmark_validdata_txt_path, args.landmark_validdata_dir)\n",
    "        if net_name == 'pnet':\n",
    "            IDS = InplaceDataset(img_face_landmark, img_faces, cropsize=12)\n",
    "        elif net_name == 'rnet':\n",
    "            IDS = InplaceDataset(img_face_landmark, img_faces, cropsize=24, pnet=load_net(args, 'pnet'))\n",
    "        elif net_name == 'onet':\n",
    "            IDS = InplaceDataset(img_face_landmark, img_faces, cropsize=48,\n",
    "                                 pnet=load_net(args, 'pnet'), rnet=load_net(args, 'rnet'))\n",
    "    return DataLoader(IDS,\n",
    "                      batch_size=args.batch_size,\n",
    "                      shuffle=True,\n",
    "                      num_workers=args.num_workers,\n",
    "                      pin_memory=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 核心model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class P_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(P_Net, self).__init__()\n",
    "        self.pre_layer = nn.Sequential(\n",
    "            # 12x12x3\n",
    "            nn.Conv2d(3, 10, kernel_size=3, stride=1),  # conv1\n",
    "            nn.PReLU(),  # PReLU1\n",
    "            # 10x10x10\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # pool1\n",
    "            # 5x5x10\n",
    "            nn.Conv2d(10, 16, kernel_size=3, stride=1),  # conv2\n",
    "            # 3x3x16\n",
    "            nn.PReLU(),  # PReLU2\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1),  # conv3\n",
    "            # 1x1x32\n",
    "            nn.PReLU()  # PReLU3\n",
    "        )\n",
    "        # detection\n",
    "        self.conv4_1 = nn.Conv2d(32, 2, kernel_size=1, stride=1)\n",
    "        # bounding box regresion\n",
    "        self.conv4_2 = nn.Conv2d(32, 4, kernel_size=1, stride=1)\n",
    "        # landmark localization\n",
    "        self.conv4_3 = nn.Conv2d(32, 10, kernel_size=1, stride=1)\n",
    "        # weight initiation with xavier\n",
    "        self.apply(weights_init)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pre_layer(x)\n",
    "        det=torch.sigmoid(self.conv4_1(x))\n",
    "        #det = F.softmax(self.conv4_1(x))\n",
    "        box = self.conv4_2(x)\n",
    "        #landmark = self.conv4_3(x)\n",
    "        # det:[,2,1,1], box:[,4,1,1], landmark:[,10,1,1]\n",
    "        return det, box#, landmark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class R_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(R_Net, self).__init__()\n",
    "        self.pre_layer = nn.Sequential(\n",
    "            # 24x24x3\n",
    "            nn.Conv2d(3, 28, kernel_size=3, stride=1),  # conv1\n",
    "            nn.PReLU(),  # prelu1\n",
    "            # 22x22x28\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),  # pool1\n",
    "            # 10x10x28\n",
    "            nn.Conv2d(28, 48, kernel_size=3, stride=1),  # conv2\n",
    "            nn.PReLU(),  # prelu2\n",
    "            # 8x8x48\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),  # pool2\n",
    "            # 3x3x48\n",
    "            nn.Conv2d(48, 64, kernel_size=2, stride=1),  # conv3\n",
    "            # 2x2x64\n",
    "            nn.PReLU()  # prelu3\n",
    "        )\n",
    "        # 2x2x64\n",
    "        self.conv4 = nn.Linear(64 * 2 * 2, 128)  # conv4\n",
    "        # 128\n",
    "        self.prelu4 = nn.PReLU()  # prelu4\n",
    "        # detection\n",
    "        self.conv5_1 = nn.Linear(128, 2)\n",
    "        # bounding box regression\n",
    "        self.conv5_2 = nn.Linear(128, 4)\n",
    "        # lanbmark localization\n",
    "        self.conv5_3 = nn.Linear(128, 10)\n",
    "        # weight initiation weih xavier\n",
    "        self.apply(weights_init)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pre_layer(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.conv4(x)\n",
    "        x = self.prelu4(x)\n",
    "        #det = torch.sigmoid(self.conv5_1(x))\n",
    "        det = F.softmax(self.conv5_1(x), dim=1)#也可以试试softmax呀\n",
    "        box = self.conv5_2(x)\n",
    "        #landmark = self.conv5_3(x)\n",
    "        return det, box#, landmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## O-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class O_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(O_Net, self).__init__()\n",
    "        self.pre_layer = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=1),  # conv1\n",
    "            nn.PReLU(),  # prelu1\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),  # pool1\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1),  # conv2\n",
    "            nn.PReLU(),  # prelu2\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),  # pool2\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),  # conv3\n",
    "            nn.PReLU(),  # prelu3\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # pool3\n",
    "            nn.Conv2d(64, 128, kernel_size=2, stride=1),  # conv4\n",
    "            nn.PReLU()  # prelu4\n",
    "        )\n",
    "        self.conv5 = nn.Linear(128 * 2 * 2, 256)  # conv5\n",
    "        self.prelu5 = nn.PReLU()  # prelu5\n",
    "        # detection\n",
    "        self.conv6_1 = nn.Linear(256, 2)\n",
    "        # bounding box regression\n",
    "        self.conv6_2 = nn.Linear(256, 4)\n",
    "        # lanbmark localization\n",
    "        self.conv6_3 = nn.Linear(256, 10)\n",
    "        # weight initiation weih xavier\n",
    "        self.apply(weights_init)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pre_layer(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.conv5(x)\n",
    "        x = self.prelu5(x)\n",
    "        # detection\n",
    "        det = F.softmax(self.conv6_1(x))\n",
    "        box = self.conv6_2(x)\n",
    "        landmark = self.conv6_3(x)\n",
    "        return det, box, landmark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用的loss-MTCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossFn:\n",
    "    def __init__(self, cls_factor=1, box_factor=1, landmark_factor=1):\n",
    "        # loss function\n",
    "        self.cls_factor = cls_factor\n",
    "        self.box_factor = box_factor\n",
    "        self.land_factor = landmark_factor\n",
    "        self.loss_box = nn.MSELoss()\n",
    "        # mean square error\n",
    "        self.loss_landmark = nn.MSELoss()\n",
    "        self.num_keep_radio = 0.7 # mini-batch前70%做为困难样本\n",
    "\n",
    "    # face/non-face 损失，注意在线困难样本挖掘（前70%）\n",
    "    def cls_loss(self, gt_label,prob_label):\n",
    "        zeros = torch.zeros_like(gt_label)\n",
    "        #只把pos的label设定为1,其余都为0\n",
    "        temp=torch.where(torch.lt(gt_label,0), zeros, gt_label)\n",
    "        label_filter_invalid =[]\n",
    "        for i in temp:\n",
    "            label_filter_invalid.append(i[0])\n",
    "        #类别size[2*batch]\n",
    "        num_cls_prob = torch.numel(prob_label)\n",
    "        cls_prob_reshape = torch.reshape(prob_label,[num_cls_prob,-1]).tolist()\n",
    "        label_int = torch.tensor(label_filter_invalid)\n",
    "        #获取batch数\n",
    "        num_row = list(prob_label.size())[0]\n",
    "        #row = [0,2,4.....]\n",
    "        #对应某一batch而言，batch*2为非人类别概率，batch*2+1为人概率类别,indices为对应 cls_prob_reshpae\n",
    "        #应该的真实值，后续用交叉熵计算损失\n",
    "        row=torch.range(0,num_row-1)*2\n",
    "        indices_ = row + label_int\n",
    "        indices_=indices_.tolist()\n",
    "        label_prob=[]\n",
    "        for i in indices_:\n",
    "            label_prob.append(cls_prob_reshape[int(i)][0])\n",
    "        #真实标签对应的概率\n",
    "        label_prob=torch.tensor(label_prob)\n",
    "        loss = -torch.log(label_prob+1e-10)\n",
    "        # get the number of POS and NEG examples\n",
    "        gt_label = torch.squeeze(gt_label)\n",
    "        mask = torch.ge(gt_label,0)\n",
    "        num_valid = torch.sum(mask)\n",
    "        ###### 困难样本数量 #####\n",
    "        keep_num = num_valid*self.num_keep_radio\n",
    "        keep_num=keep_num.int()\n",
    "        #FILTER OUT PART AND LANDMARK DATA\n",
    "        loss = loss * mask\n",
    "        loss,_ = torch.topk(loss, k=keep_num) ##### 仅取困难样本反向传播 #####\n",
    "        #print(loss)\n",
    "        return torch.mean(loss)*self.cls_factor\n",
    "\n",
    "\n",
    "    def box_loss(self, gt_label, gt_offset, pred_offset):\n",
    "        # if gt_label is torch.tensor([0.0]):\n",
    "        #     return torch.tensor([0.0])\n",
    "        # pred_offset: [batch_size, 4] to [batch_size,4]\n",
    "        pred_offset = torch.squeeze(pred_offset)\n",
    "        # gt_offset: [batch_size, 4, 1, 1] to [batch_size,4]\n",
    "        gt_offset = torch.squeeze(gt_offset)\n",
    "        # gt_label: [batch_size, 1, 1, 1] to [batch_size]\n",
    "        gt_label = torch.squeeze(gt_label)\n",
    "\n",
    "        # get the mask element which != 0\n",
    "        # unmask = torch.eq(gt_label, 0)\n",
    "        # mask = torch.eq(unmask, 0)\n",
    "        mask = torch.eq(gt_label, 1)\n",
    "        # convert mask to dim index\n",
    "        \n",
    "        valid_gt_offset = gt_offset[mask, :]\n",
    "        valid_pred_offset = pred_offset[mask, :]\n",
    "        # print('valid_gt_offset', valid_gt_offset, 'valid_pred_offset', valid_pred_offset)\n",
    "        valid_sample_num = valid_gt_offset.shape[0]\n",
    "        if 0 == valid_sample_num:\n",
    "            # print('No box')\n",
    "            # return self.loss_box(torch.tensor([0.0]), torch.tensor([0.0]))\n",
    "            return torch.tensor([0.0])\n",
    "        else:\n",
    "            # print('valid_sample_num', valid_sample_num)\n",
    "            return self.loss_box(valid_pred_offset, valid_gt_offset) * self.box_factor\n",
    "        # return torch.tensor([0.])\n",
    "\n",
    "    def landmark_loss(self, landmark_flag, gt_landmark=None, pred_landmark=None):\n",
    "        if pred_landmark==None:\n",
    "            return torch.tensor([0.0])\n",
    "        # pred_landmark:[batch_size,10,1,1] to [batch_size,10]\n",
    "        pred_landmark = torch.squeeze(pred_landmark)\n",
    "        # gt_landmark:[batch_size,10] to [batch_size,10]\n",
    "        gt_landmark = torch.squeeze(gt_landmark)\n",
    "        # gt_label:[batch_size,1] to [batch_size]\n",
    "        gt_label = torch.squeeze(landmark_flag)\n",
    "        mask = torch.eq(gt_label, 1)\n",
    "        valid_gt_landmark = gt_landmark[mask, :]\n",
    "        valid_pred_landmark = pred_landmark[mask, :]\n",
    "        valid_sample_num = valid_gt_landmark.shape[0]\n",
    "        if 0 == valid_sample_num:\n",
    "            return torch.tensor([0.0])\n",
    "        else:\n",
    "            return self.loss_landmark(valid_pred_landmark, valid_gt_landmark) * self.land_factor\n",
    "\n",
    "    def total_loss(self, gt_label, pred_label, gt_offset, pred_offset, landmark_flag, gt_landmark, pred_landmark):\n",
    "        return self.cls_loss(gt_label, pred_label) \\\n",
    "               + self.box_loss(gt_label, gt_offset, pred_offset) \\\n",
    "               + self.landmark_loss(landmark_flag, gt_landmark, pred_landmark)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train 部分的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # 1. 根据网络层的不同定义不同的初始化方式     \n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_normal_(m.weight)\n",
    "        #nn.init.constant_(m.bias, 0) bias不要全初始化为0\n",
    "        nn.init.normal_(m.bias, mean=0, std=1)\n",
    "    # 也可以判断是否为conv2d，使用相应的初始化方式 \n",
    "    elif isinstance(m, nn.Conv2d):\n",
    "        nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "     # 是否为批归一化层\n",
    "    elif isinstance(m, nn.BatchNorm2d):\n",
    "        nn.init.constant_(m.weight, 1)\n",
    "        nn.init.constant_(m.bias, 0)\n",
    "#使用这样的初始化后，模型的初始表现确实好了些"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_net(args, net_name):\n",
    "    # pnet, rnet, onet = P_Net(), R_Net(), O_Net()\n",
    "    net_list = {'pnet': P_Net(), 'rnet': R_Net(), 'onet': O_Net()}\n",
    "    try:\n",
    "        net = net_list[net_name].to(device)\n",
    "        try:\n",
    "            print('===> loading the saved net weights...')\n",
    "            _ = osp.join(args.save_folder, net_name + '.pkl')\n",
    "            print('===> check {} saved path({}):{}'.format(net_name, _, osp.exists(_)))\n",
    "            net.load_state_dict(torch.load(_, map_location=device))\n",
    "            return net  # , rnet, onet\n",
    "        except Exception:\n",
    "            print('*** fail to load the saved net weights!')\n",
    "            return net\n",
    "    except Exception:\n",
    "        print('*** Net name wrong!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_para(file_name):\n",
    "    # para = None\n",
    "    try:\n",
    "        print('===> loading the saved parameters...')\n",
    "        para = torch.load(osp.join(args.save_folder, file_name))\n",
    "    except Exception:\n",
    "        print('*** fail to load the saved parameters!')\n",
    "        print('===> initailizing the parameters...')\n",
    "        para = {\n",
    "            'lr': args.lr,\n",
    "            'iter': 0,\n",
    "            'loss': [],\n",
    "            'val_result': [],\n",
    "            'optimizer_param': None\n",
    "        }\n",
    "        save_safely(para, dir_path=args.save_folder, file_name=file_name)\n",
    "    return para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_safely(file, dir_path, file_name):\n",
    "    if not osp.exists(dir_path):\n",
    "        os.mkdir(dir_path)\n",
    "        print('*** dir not exist, created one')\n",
    "    save_path = osp.join(dir_path, file_name)\n",
    "    if osp.exists(save_path):\n",
    "        temp_name = save_path + '.temp'\n",
    "        torch.save(file, temp_name)\n",
    "        os.remove(save_path)\n",
    "        os.rename(temp_name, save_path)\n",
    "        #print('*** find the file conflict while saving, saved safely')\n",
    "    else:\n",
    "        torch.save(file, save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_net(args, net_name='pnet',loss_config=[]):\n",
    "    net = load_net(args, net_name)\n",
    "    para = load_para(net_name + '_para.pkl')\n",
    "    lr = para['lr']\n",
    "    iter_count = para['iter']\n",
    "    optimizer = opt.SGD(net.parameters(), lr=args.lr, momentum=0.9, dampening=0, weight_decay=0, nesterov=False)\n",
    "    loss = LossFn(cls_factor=loss_config[0], box_factor=loss_config[1], landmark_factor=loss_config[2])\n",
    "    if para['optimizer_param'] is not None:\n",
    "        optimizer.state_dict()['param_groups'][0].update(para['optimizer_param'])\n",
    "        print('===> updated the param of optimizer.')\n",
    "    #导入dataloader\n",
    "    train_loader = get_inplace_data_loader(args, net_name,data=\"train\")\n",
    "    valid_loader = get_inplace_data_loader(args, net_name,data=\"valid\")\n",
    "    # to track the training loss as the model trains\n",
    "    train_losses = []\n",
    "    # to track the validation loss as the model trains\n",
    "    valid_losses = []\n",
    "    # to track the average training loss per epoch as the model trains\n",
    "    avg_train_losses = []\n",
    "    # to track the average validation loss per epoch as the model trains\n",
    "    avg_valid_losses = [] \n",
    "    \n",
    "    for i in range(args.epoch):\n",
    "        print('Epoch {}/{}'.format(i+1, args.epoch))\n",
    "        print('-' * 10)\n",
    "        t0 = time.perf_counter()\n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        net.train()\n",
    "        for step, (img_tensor, label, offset, landmark_flag, landmark) in enumerate(train_loader):\n",
    "            iter_count += 1\n",
    "            # update lr rate\n",
    "            wrap = (img_tensor, label, offset, landmark)\n",
    "            (img_tensor, label, offset, landmark) = [i.to(device) for i in wrap]\n",
    "            if net_name==\"pnet\"or net_name==\"rnet\":\n",
    "                det, box = net(img_tensor)\n",
    "                ldmk=None\n",
    "            elif net_name==\"onet\":\n",
    "                det, box, ldmk = net(img_tensor)\n",
    "            optimizer.zero_grad()\n",
    "            # print('offset:', offset)\n",
    "            all_loss = loss.total_loss(gt_label=label, pred_label=det, gt_offset=offset, pred_offset=box,\n",
    "                                       landmark_flag=landmark_flag, pred_landmark=ldmk, gt_landmark=landmark)\n",
    "            \n",
    "            all_loss.requires_grad_()\n",
    "            all_loss.backward()\n",
    "            optimizer.step()\n",
    "            train_losses.append(all_loss.item())\n",
    "            # 判断是否保存参数\n",
    "            if 0 == iter_count % args.save_steps:\n",
    "                if 0 == iter_count % args.half_lr_steps:\n",
    "                    lr /= 2\n",
    "                    para.update({'lr': lr})\n",
    "                    for param_groups in optimizer.param_groups:\n",
    "                        param_groups['lr'] = lr\n",
    "                    print('*** lr updated:{}'.format(lr))\n",
    "                para.update({\n",
    "                    'lr': lr,\n",
    "                    'iter': iter_count,\n",
    "                    'optimizer_param': optimizer.state_dict()['param_groups'][0]\n",
    "                })\n",
    "                save_safely(net.state_dict(), args.save_folder, net_name + '.pkl')\n",
    "                save_safely(para, args.save_folder, net_name + '_para.pkl')    \n",
    "        ######################    \n",
    "        # validate the model #\n",
    "        ######################\n",
    "        net.eval()\n",
    "        for step, (img_tensor, label, offset, landmark_flag, landmark) in enumerate(valid_loader):\n",
    "            # update lr rate\n",
    "            wrap = (img_tensor, label, offset, landmark)\n",
    "            (img_tensor, label, offset, landmark) = [i.to(device) for i in wrap]\n",
    "            if net_name==\"pnet\"or net_name==\"rnet\":\n",
    "                det, box = net(img_tensor)\n",
    "                ldmk=None\n",
    "            elif net_name==\"onet\":\n",
    "                det, box, ldmk = net(img_tensor)\n",
    "            optimizer.zero_grad()\n",
    "            # print('offset:', offset)\n",
    "            all_loss = loss.total_loss(gt_label=label, pred_label=det, gt_offset=offset, pred_offset=box,\n",
    "                                       landmark_flag=landmark_flag, pred_landmark=ldmk, gt_landmark=landmark)\n",
    "            valid_losses.append(all_loss.item())\n",
    "            t1 = time.perf_counter()\n",
    "            \n",
    "            \n",
    "        # print training/validation statistics \n",
    "        # calculate average loss over an epoch\n",
    "        train_loss = np.average(train_losses)\n",
    "        valid_loss = np.average(valid_losses)\n",
    "        avg_train_losses.append(train_loss)\n",
    "        avg_valid_losses.append(valid_loss)\n",
    "        print_msg = (f'train_loss: {train_loss:.5f} '+f'|   valid_loss: {valid_loss:.5f} ')\n",
    "        print(print_msg)\n",
    "        # clear lists to track next epoch\n",
    "        train_losses = []\n",
    "        valid_losses = []   \n",
    "        #print这一epoch的时间和lr\n",
    "        t1 = time.perf_counter()\n",
    "        print('===> iter:{}\\t|lr:{:.12f} | time:{:.8f}'.format(iter_count, lr, t1 - t0))\n",
    "    return avg_train_losses, avg_valid_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 开始训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.训练P-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss, valid_loss=train_net(args, net_name='pnet',loss_config=net_loss_config['pnet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_show(\"pnet\",train_loss,valid_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imageshow(args,data=\"train\",num_images=8,Random=False,net_name='pnet',save_name=\"pnet-try-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.训练R-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss, valid_loss=train_net(args, net_name='rnet',loss_config=net_loss_config['rnet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_show(\"rnet\",train_loss,valid_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imageshow(args,data=\"train\",num_images=8,Random=False,net_name='rnet',save_name=\"rnet-try-2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练O-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss,valid_loss=train_net(args, net_name='onet',loss_config=net_loss_config['onet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_show(\"onet\",train_loss,valid_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imageshow(args,data=\"train\",num_images=8,Random=False,net_name='onet',save_name=\"onet-try-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 开始检测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pnet= load_net(args, 'pnet')\n",
    "rnet= load_net(args, 'rnet')\n",
    "onet= load_net(args, 'onet')\n",
    "mtcnn_detector = MtcnnDetector(pnet=pnet,rnet=rnet,onet=onet,min_face_size=12)\n",
    "img = Image.open(\"1.jpg\")\n",
    "bboxs, landmarks = mtcnn_detector.detect_face(img)\n",
    "# print box_align\n",
    "save_name = 'r_1.jpg'\n",
    "vis_face(img,bboxs,landmarks, save_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
